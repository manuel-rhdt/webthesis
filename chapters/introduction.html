<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <link rel="sitemap" type="application/xml" href="/sitemap.xml">
  <title>Introduction</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../css/thesis-style.css" />
  <a href="../index.html" class="back-link">â† Back to Contents</a>
  <script src="../js/equation-tooltips.js"></script>
</head>
<body data-chapter="1">
<header id="title-block-header">
<h1 class="title">Introduction</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#contributions-of-this-work"
id="toc-contributions-of-this-work">Contributions of This Work</a></li>
<li><a href="#thesis-outline" id="toc-thesis-outline">Thesis
Outline</a></li>
</ul></li>
<li><a href="#bibliography" id="toc-bibliography">References</a></li>
</ul>
</nav>
<h2 id="abstract">Abstract</h2>
<blockquote>
<p>The fundamental problem of communication is that of reproducing at
one point either exactly or approximately a message selected at another
point. Frequently the messages have meaning; that is they refer to or
are correlated according to some system with certain physical or
conceptual entities. These semantic aspects of communication are
irrelevant to the engineering problem. The significant aspect is that
the actual message is one selected from a set of possible messages. The
system must be designed to operate for each possible selection, not just
the one which will actually be chosen since this is unknown at the time
of design.</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>We live in the era of information. Information technology permeates
every aspect of modern life, shaping how we communicate, learn, have
social interactions, and spend our leisure time. Beyond daily life,
information plays a crucial role in fields like physics, biology,
neuroscience, and engineering, where it is used to study and enhance the
function of complex systems and machines. Quantifying the flow of
information within these domains is essential, and, although the concept
of information is abstract, its power in explaining the processes that
shape our world is profound.</p>
<p>The explanatory power of information stems from the intrinsic link
between information and performance. Without a potential reward, or the
possibility of avoiding harm, information has no value.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> As
a result, information collection and processing typically serves a clear
purpose. For example, a self-driving car processes information from its
sensors in order to make decisions about navigation <span
class="citation" data-cites="2018.Schwarting">Â [<a
href="#ref-2018.Schwarting" role="doc-biblioref">3</a>]</span>.
Similarly, bacteria acquire chemical information about their environment
in order to optimize their movement toward nutrients and away from
toxins, maximizing their chance of survival <span class="citation"
data-cites="2021.Mattingly">Â [<a href="#ref-2021.Mattingly"
role="doc-biblioref">4</a>]</span>. More generally, in evolutionary
biology the link between genetic information and fitness is explored
<span class="citation" data-cites="2012.Adami 2022.Hledik">Â [<a
href="#ref-2012.Adami" role="doc-biblioref">5</a>,<a
href="#ref-2022.Hledik" role="doc-biblioref">6</a>]</span>. Thus,
whether in biological organisms or engineered systems, understanding how
information is used is essential for optimizing performance.</p>
<p>Quantifying information transmission is vital for understanding and
improving natural or engineered information-processing systems.
Shannonâ€™s <em>information theory</em> <span class="citation"
data-cites="1948.Shannon">Â [<a href="#ref-1948.Shannon"
role="doc-biblioref">7</a>]</span> provides the framework for studying
the efficiency and reliability of any communication channel, whether
itâ€™s a telephone line, a biochemical signaling cascade, or a neural
pathway in the brain. The cornerstone of information theory is a set of
mathematical definitions to rigorously quantify amounts of information.
These makes it possible to determine, in absolute terms, the amount of
information that is transmitted by a given information-processing
mechanism, for a specific input signal. Moreover, it is possible to
quantify the maximum amount of information that can be transmitted
through a given mechanism under optimal conditions: this limit is known
as the channel capacity, measured in bits per time unit. Shannonâ€™s
information measures enable us to characterize a wide range of systems
in terms of their information transmission capabilities.</p>
<p>Information theory has found many applications across disciplines,
and is frequently used to understand and improve sensory or
computational systems. In biology, information transmission is studied,
e.g., in the brain, by analyzing the timing of electrical impulses
between neurons <span class="citation"
data-cites="1998.Strong 1999.Rieke">Â [<a href="#ref-1998.Strong"
role="doc-biblioref">8</a>,<a href="#ref-1999.Rieke"
role="doc-biblioref">9</a>]</span>. Within cells, information flow in
biochemical signaling and transcription regulation has been extensively
studied by analyzing biochemical pathways <span class="citation"
data-cites="2008.Tkacik 2009.Mehta 2011.Cheong">Â [<a
href="#ref-2008.Tkacik" role="doc-biblioref">10</a>â€“<a
href="#ref-2011.Cheong" role="doc-biblioref">12</a>]</span>. In
artificial intelligence, information theory has proven useful in
improving learning in neural networks. The information bottleneck theory
<span class="citation" data-cites="2000.Tishby">Â [<a
href="#ref-2000.Tishby" role="doc-biblioref">13</a>]</span> suggests
that the performance of neural networks can be enhanced by balancing
compression and information retention during training <span
class="citation" data-cites="2015.Tishby 2017.Schwartz-Ziv">Â [<a
href="#ref-2015.Tishby" role="doc-biblioref">14</a>,<a
href="#ref-2017.Schwartz-Ziv" role="doc-biblioref">15</a>]</span>. In
economics and finance, information theory has been applied to describe
financial markets <span class="citation" data-cites="2014.Fiedor">Â [<a
href="#ref-2014.Fiedor" role="doc-biblioref">16</a>]</span> and to
optimize financial decision-making under uncertainty <span
class="citation" data-cites="1956.Kelly">Â [<a href="#ref-1956.Kelly"
role="doc-biblioref">17</a>]</span>. In optics, information theory is
employed to study the efficiency of signal processing in optical
resonators, with applications in precision sensing and optical computing
<span class="citation" data-cites="2014.Aspelmeyer 2022.Peters">Â [<a
href="#ref-2014.Aspelmeyer" role="doc-biblioref">18</a>,<a
href="#ref-2022.Peters" role="doc-biblioref">19</a>]</span>. Information
theory boasts a wealth of applications and is essential for the analysis
and theoretical understanding of information-processing systems.</p>
<p>The canonical measure for the quality of information transmission is
the mutual information. It quantifies how much information is shared
between two random variables, such as the input and output signals of an
information-processing mechanism, see <a
href="#fig:information_transmission_flow">Fig.
information_transmission_flow</a>. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
be two random variables that are jointly distributed according to the
density
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(s, x)</annotation></semantics></math>
and with marginal densities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(s)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(x)</annotation></semantics></math>.
The mutual information between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is then defined as </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>âˆ¬</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>s</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">I(S, X) = \iint \mathrm{P}(s, x) \ln\left(
\frac{\mathrm{P}(s, x)}{\mathrm{P}(s)\mathrm{P}(x)}
\right)ds\,dx</annotation></semantics></math>
<p> and provides a measure of correlation between the random
variables.<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> From the definition it follows that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">I(S, X)=0</annotation></semantics></math>
only if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are statistically independent, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">I(S, X)&gt;0</annotation></semantics></math>
otherwise. Thus, the mutual information quantifies the statistical
dependence between random variables, equally characterizing the degrees
of influence from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>â†’</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">S \to X</annotation></semantics></math>
and from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>â†’</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">X \to S</annotation></semantics></math>.
Hence, the mutual information is a symmetric measure, satisfying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo>,</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S,X)=I(X,S)</annotation></semantics></math>.
In a typical information processing system, the input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
influences the output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
but there is no feedback from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>.
In such cases, the mutual information
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S, X)</annotation></semantics></math>
provides a measure for how effectively information about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is transmitted through the system into the
outputÂ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.</p>
<figure id="fig:information_transmission_flow">
<img src="../images/information_transmission_flow.svg" style="width:65.0%"  />
<figcaption>A generic information processing device takes an input
signal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
and produces an output signal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.
Because the output is correlated with the input, we can quantify the
information that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
share. This quantity is called the mutual information and measures the
information that is transmitted.</figcaption>
</figure>
<p>In biological systems, information transmission has frequently been
quantified via the <em>instantaneous mutual information</em> (IMI)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><msub><mi>t</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>X</mi><msub><mi>t</mi><mn>2</mn></msub></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S_{t_1}, X_{t_2})</annotation></semantics></math>,
i.e. the mutual information between stimulus and response at two time
points. This measure has been applied for analyzing biochemical pathways
<span class="citation"
data-cites="2009.Tkacik 2010.Walczak 2011.Cheong 2012.Bowsher 2014.Clausznitzer 2015.Palmer 2016.Pilkiewicz">Â [<a
href="#ref-2011.Cheong" role="doc-biblioref">12</a>,<a
href="#ref-2016.Pilkiewicz" role="doc-biblioref">22</a>,<a
href="#ref-2009.Tkacik" role="doc-biblioref">25</a>â€“<a
href="#ref-2015.Palmer" role="doc-biblioref">29</a>]</span> and neural
spiking dynamics <span class="citation"
data-cites="1998.Strong 1999.Borst">Â [<a href="#ref-1998.Strong"
role="doc-biblioref">8</a>,<a href="#ref-1999.Borst"
role="doc-biblioref">30</a>]</span>. However, in many cases, the IMI
cannot correctly quantify information transmission due to correlations
within the input or the output which reduce the total information
transmitted. More generally, information may be encoded in the temporal
patterns of signals, which cannot be captured by a pointwise information
measure like the IMI. Thus, the IMI is generally inadequate for
computing information transmission in systems which process dynamical
signals.</p>
<p>There are many examples of information being encoded in dynamical
features of signals. In cellular Ca<sup>2+</sup> signaling, information
seems to be encoded in the timing and duration of calcium bursts <span
class="citation" data-cites="2008.Boulware">Â [<a
href="#ref-2008.Boulware" role="doc-biblioref">31</a>]</span>, while in
the MAPK pathway information is encoded in the amplitude and duration of
the transient phosphorylation response to external stimuli <span
class="citation" data-cites="2018.Mitra 2023.Nalecz-Jawecki">Â [<a
href="#ref-2018.Mitra" role="doc-biblioref">32</a>,<a
href="#ref-2023.Nalecz-Jawecki" role="doc-biblioref">33</a>]</span>.
Moreover, there are reasons to believe that encoding information in
dynamical signal features is advantageous for reliable information
transmission <span class="citation" data-cites="2014.Selimkhanov">Â [<a
href="#ref-2014.Selimkhanov" role="doc-biblioref">34</a>]</span>.
Studying the information transmitted via temporal features is thus
highly desirable but not possible with an instantaneous information
measure. Therefore, in cases where the dynamics of input or output
time-series may carry relevant information, the need for appropriate
dynamical information measures has been widely recognized <span
class="citation"
data-cites="2008.Staniek 2009.Tostevin 2012.Runge 2021.Meijers 2021.Tang 2021.Mattingly 2023.Nalecz-Jawecki 2023.Hahn 2024.Umeki 2024.Nicoletti">Â [<a
href="#ref-2021.Mattingly" role="doc-biblioref">4</a>,<a
href="#ref-2023.Nalecz-Jawecki" role="doc-biblioref">33</a>,<a
href="#ref-2008.Staniek" role="doc-biblioref">35</a>â€“<a
href="#ref-2024.Nicoletti" role="doc-biblioref">42</a>]</span>.</p>
<p>The natural measure for quantifying information transmission via
dynamical signals is the <em>trajectory mutual information</em>. It
takes into account the total information encoded in the input and output
trajectories of a system, and therefore captures all information
transmitted over a specific time interval. Conceptually, its definition
is simple. The trajectory mutual information is the mutual information
between the input and output trajectories of a stochastic process, given
by </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘º</mi><mo>,</mo><mi>ğ‘¿</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>âˆ¬</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo>,</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo>,</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>ğ’”</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>ğ’™</mi></mrow><annotation encoding="application/x-tex">I(\mathbfit{S}, \mathbfit{X}) = \iint \mathrm{P}(\mathbfit{s}, \mathbfit{x}) \ln\left(
\frac{\mathrm{P}(\mathbfit{s}, \mathbfit{x})}{\mathrm{P}(\mathbfit{s})\mathrm{P}(\mathbfit{x})}
\right)d\mathbfit{s}\,d\mathbfit{x}</annotation></semantics></math>
<p> where the bold symbols
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’”</mi><annotation encoding="application/x-tex">\mathbfit{s}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’™</mi><annotation encoding="application/x-tex">\mathbfit{x}</annotation></semantics></math>
are used to denote trajectories. These trajectories arise from a
stochastic process that defines the joint probability distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo>,</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{s}, \mathbfit{x})</annotation></semantics></math>.
The integral itself runs over all possible input and output
trajectories.</p>
<p>The closely related <em>mutual information rate</em> is defined as
the rate at which the trajectory mutual information increases with the
duration of the trajectories in the long-time limit. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ğ‘º</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\mathbfit{S}_T</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ğ‘¿</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\mathbfit{X}_T</annotation></semantics></math>
be trajectories of duration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>,
then the mutual information rate is given by </p>
<div id="eq:intro-info-rate">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><munder><mi>lim</mi><mrow><mi>T</mi><mo>â†’</mo><mi>âˆ</mi></mrow></munder><mfrac><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mi>T</mi></mfrac><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">R = \lim_{T\to\infty} \frac{I(\mathbfit{S}_T, \mathbfit{X}_T)}{T} \,.
    \label{eq:intro-info-rate}</annotation></semantics></math>
</div>
<p> The mutual information rate quantifies how many independent messages
can be transmitted per unit time, on average, via a communication
channel. It depends on both, the signal statistics of the input, as well
as the transmission properties of the channel. In the absence of
feedback it is equal to the transfer entropy <span class="citation"
data-cites="2000.Schreiber 2002.Kaiser">Â [<a href="#ref-2000.Schreiber"
role="doc-biblioref">43</a>,<a href="#ref-2002.Kaiser"
role="doc-biblioref">44</a>]</span>.</p>
<p>The trajectory mutual information and the mutual information rate are
fundamental measures for information transmission in dynamical systems.
They serve as key performance metrics for biochemical signaling networks
<span class="citation" data-cites="2009.Tostevin 2011.Cheong">Â [<a
href="#ref-2011.Cheong" role="doc-biblioref">12</a>,<a
href="#ref-2009.Tostevin" role="doc-biblioref">36</a>]</span>, as well
as for neural sensory systems <span class="citation"
data-cites="1998.Strong 1999.Borst">Â [<a href="#ref-1998.Strong"
role="doc-biblioref">8</a>,<a href="#ref-1999.Borst"
role="doc-biblioref">30</a>]</span>. More generally, in communication
channels with memory, the mutual information rate for the optimal input
signal determines the channel capacity <span class="citation"
data-cites="2006.Cover">Â [<a href="#ref-2006.Cover"
role="doc-biblioref">20</a>]</span>. In financial markets, it quantifies
correlations in stochastic time series, such as stock prices and trading
volumes <span class="citation" data-cites="2014.Fiedor">Â [<a
href="#ref-2014.Fiedor" role="doc-biblioref">16</a>]</span>. Finally, in
non-equilibrium thermodynamics, the trajectory mutual information
provides a link between information theory and stochastic thermodynamics
<span class="citation" data-cites="2013.Barato 2014.Hartich">Â [<a
href="#ref-2013.Barato" role="doc-biblioref">45</a>,<a
href="#ref-2014.Hartich" role="doc-biblioref">46</a>]</span>. Efficient
methods for calculating the trajectory mutual information and the mutual
information rate are needed and constitute the primary objective of this
thesis.</p>
<p>Unfortunately, calculating the mutual information between
trajectories is notoriously difficult due to the high dimensionality of
trajectory space <span class="citation" data-cites="2003.Paninski">Â [<a
href="#ref-2003.Paninski" role="doc-biblioref">47</a>]</span>.
Conventional approaches for computing mutual information require
non-parametric estimates of the input and output entropy, typically
obtained via histograms or kernel density estimators <span
class="citation"
data-cites="1998.Strong 2003.Paninski 2011.Cheong 2008.Tkacik 2014.Tkacik 2021.Meijers">Â [<a
href="#ref-1998.Strong" role="doc-biblioref">8</a>,<a
href="#ref-2008.Tkacik" role="doc-biblioref">10</a>,<a
href="#ref-2011.Cheong" role="doc-biblioref">12</a>,<a
href="#ref-2021.Meijers" role="doc-biblioref">38</a>,<a
href="#ref-2003.Paninski" role="doc-biblioref">47</a>,<a
href="#ref-2014.Tkacik" role="doc-biblioref">48</a>]</span>. However,
the high-dimensional nature of trajectories makes it infeasible to
obtain enough data for accurate non-parametric distribution estimates.
Other non-parametric entropy estimators such as the k-nearest-neighbor
estimator <span class="citation"
data-cites="2002.Kaiser 2004.Kraskov">Â [<a href="#ref-2002.Kaiser"
role="doc-biblioref">44</a>,<a href="#ref-2004.Kraskov"
role="doc-biblioref">49</a>]</span> depend on a choice of metric in
trajectory space and become unreliable for long trajectories <span
class="citation" data-cites="2019.Cepeda-Humerez">Â [<a
href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">50</a>]</span>.
Thus, except for very simple systems <span class="citation"
data-cites="2021.Meijers">Â [<a href="#ref-2021.Meijers"
role="doc-biblioref">38</a>]</span>, the curse of dimensionality makes
it infeasible to obtain accurate results for the trajectory mutual
information using conventional mutual information estimators.</p>
<p>Due to the inherent difficulty of directly estimating the mutual
information between trajectories, previous research has often employed
simplified models or approximations. In some cases, the problem can be
simplified by considering static (scalar) inputs instead of input signal
trajectories <span class="citation"
data-cites="2014.Selimkhanov 2019.Cepeda-Humerez 2021.Tang">Â [<a
href="#ref-2014.Selimkhanov" role="doc-biblioref">34</a>,<a
href="#ref-2021.Tang" role="doc-biblioref">39</a>,<a
href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">50</a>]</span>. But
this approach ignores the dynamics of the input signal. Lower bounds for
the mutual information can be derived from the Donsker-Varadhan
inequality <span class="citation"
data-cites="1983.Donsker 2018.Belghazi 2018.McAllester">Â [<a
href="#ref-1983.Donsker" role="doc-biblioref">51</a>â€“<a
href="#ref-2018.McAllester" role="doc-biblioref">53</a>]</span>, or
obtained through general-purpose compression algorithms <span
class="citation"
data-cites="2005.Baronchelli 2008.Gao 2019.Cepeda-Humerez">Â [<a
href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">50</a>,<a
href="#ref-2005.Baronchelli" role="doc-biblioref">54</a>,<a
href="#ref-2008.Gao" role="doc-biblioref">55</a>]</span>. While exact
analytical results for the trajectory mutual information are available
for certain simple processes such as Gaussian <span class="citation"
data-cites="2009.Tostevin">Â [<a href="#ref-2009.Tostevin"
role="doc-biblioref">36</a>]</span> or Poisson channels <span
class="citation" data-cites="2023.Sinzger 2024.Gehri">Â [<a
href="#ref-2023.Sinzger" role="doc-biblioref">56</a>,<a
href="#ref-2024.Gehri" role="doc-biblioref">57</a>]</span>, many
complex, realistic systems lack analytical solutions, and approximations
have to be employed. For systems governed by a master equation,
numerical or analytical approximations are sometimes feasible <span
class="citation" data-cites="2019.Duso 2023.Moor">Â [<a
href="#ref-2019.Duso" role="doc-biblioref">58</a>,<a
href="#ref-2023.Moor" role="doc-biblioref">59</a>]</span> but these
become intractable for complex systems. Finally, the Gaussian framework
for approximating the mutual information rate is particularly widely
used <span class="citation"
data-cites="2009.Tostevin 2021.Mattingly 2023.Hahn">Â [<a
href="#ref-2021.Mattingly" role="doc-biblioref">4</a>,<a
href="#ref-2009.Tostevin" role="doc-biblioref">36</a>,<a
href="#ref-2023.Hahn" role="doc-biblioref">40</a>]</span>, though it
assumes linear system dynamics and Gaussian noise statistics. These
assumptions make it ill-suited for many realistic nonlinear
information-processing systems.</p>
<p>To address the limitations of previous methods, we introduce <em>Path
Weight Sampling</em> (PWS), a novel Monte Carlo technique for computing
the trajectory mutual information efficiently and accurately. PWS
leverages free-energy estimators from statistical physics and combines
analytical and numerical methods to circumvent the curse of
dimensionality associated with long trajectories. The approach relies on
exact calculations of trajectory likelihoods derived analytically from a
stochastic model. By averaging these likelihoods in a Monte Carlo
fashion, PWS can accurately compute the trajectory mutual information,
even in high-dimensional settings.</p>
<p>PWS is an exact Monte Carlo scheme, in the sense that it provides an
unbiased statistical estimate of the trajectory mutual information. In
PWS, the mutual information is computed via the identity </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘º</mi><mo>,</mo><mi>ğ‘¿</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo>âˆ£</mo><mi>ğ‘º</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(\mathbfit{S}, \mathbfit{X}) = H(\mathbfit{X}) - H(\mathbfit{X} \mid \mathbfit{S})</annotation></semantics></math>
<p> as the difference between the marginal output entropy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">H(\mathbfit{X})</annotation></semantics></math>
associated with the marginal distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x})</annotation></semantics></math>
of the output trajectories
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’™</mi><annotation encoding="application/x-tex">\mathbfit{x}</annotation></semantics></math>
and the conditional output entropy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo>âˆ£</mo><mi>ğ‘º</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">H(\mathbfit{X} \mid \mathbfit{S})</annotation></semantics></math>
associated with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}|\mathbfit{s})</annotation></semantics></math>,
the conditional output distribution for a given input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’”</mi><annotation encoding="application/x-tex">\mathbfit{s}</annotation></semantics></math>.
Both entropies are evaluated as Monte-Carlo averages over the associated
distribution, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mo stretchy="false" form="prefix">âŸ¨</mo><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">âŸ©</mo></mrow><annotation encoding="application/x-tex">H(\mathbfit{X}) = -\langle \ln \mathrm{P}(\mathbfit{x}) \rangle</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ‘¿</mi><mo>âˆ£</mo><mi>ğ‘º</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mo stretchy="false" form="prefix">âŸ¨</mo><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">âŸ©</mo></mrow><annotation encoding="application/x-tex">H(\mathbfit{X} \mid \mathbfit{S}) = -\langle \ln \mathrm{P}(\mathbfit{x}|\mathbfit{s}) \rangle</annotation></semantics></math>,
where the notation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><mi>â‹…</mi><mo stretchy="false" form="postfix">âŸ©</mo></mrow><annotation encoding="application/x-tex">\langle\cdot\rangle</annotation></semantics></math>
denotes an average with respect to the joint distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo>,</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{s}, \mathbfit{x})</annotation></semantics></math>.
The key insights of PWS are that the conditional probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}|\mathbfit{s})</annotation></semantics></math>
can be directly evaluated from a generative model of the system, and
that the marginal probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x})</annotation></semantics></math>
can be computed efficiently via marginalization using Monte Carlo
procedures inspired by computational statistical physics.</p>
<p>The crux of PWS lies in the efficient computation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x})</annotation></semantics></math>
via the marginalization integral </p>
<div id="eq:intro-marginaliztion">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>âˆ«</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>ğ’”</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}) = \int \mathrm{P}(\mathbfit{x}|\mathbfit{s}) \mathrm{P}(\mathbfit{s})\,d\mathbfit{s}\,.
    \label{eq:intro-marginaliztion}</annotation></semantics></math>
</div>
<p> To evaluate this integral efficiently, we present different variants
of PWS. In <a href="#ch:dpws">Ch. dpws</a> we introduce <em>Direct
PWS</em>, the simplest variant of PWS, where <a
href="#eq:intro-marginaliztion">eq:intro-marginaliztion</a> is computed
bia a â€œbrute-forceâ€ Monte Carlo approach that works well for short
trajectories, but which becomes exponentially harder for long
trajectories. In <a href="#ch:variants">Ch. variants</a>, we present two
additional variants of PWS that evaluate the marginalization integral
more efficiently, <em>RR-PWS</em> and <em>TI-PWS</em>.
Rosenbluth-Rosenbluth PWS (RR-PWS) is based on efficient free-energy
estimation techniques developed in polymer physics <span
class="citation"
data-cites="1955.Rosenbluth 1990.Siepmann 1997.Grassberger 2002.Frenkel">Â [<a
href="#ref-1955.Rosenbluth" role="doc-biblioref">60</a>â€“<a
href="#ref-2002.Frenkel" role="doc-biblioref">63</a>]</span>.
Thermodynamic integration PWS (TI-PWS) uses techniques from transition
path sampling to derive a MCMC sampler in trajectory space <span
class="citation" data-cites="2002.Bolhuis">Â [<a href="#ref-2002.Bolhuis"
role="doc-biblioref">64</a>]</span>. From this MCMC chain, we can
compute the marginalization integral using thermodynamic integration
<span class="citation"
data-cites="1998.Gelman 2001.Neal 2002.Frenkel">Â [<a
href="#ref-2002.Frenkel" role="doc-biblioref">63</a>,<a
href="#ref-1998.Gelman" role="doc-biblioref">65</a>,<a
href="#ref-2001.Neal" role="doc-biblioref">66</a>]</span>. Finally, in
<a href="#ch:ml-pws">Ch. ml-pws</a>, we introduce a fourth
marginalization technique based on variational inference via neural
networks <span class="citation" data-cites="2013.Kingma">Â [<a
href="#ref-2013.Kingma" role="doc-biblioref">67</a>]</span>. Its
conceptual simplicity, coupled with powerful marginalization methods,
make PWS a versatile framework for computing the trajectory mutual
information in a variety of scenarios.</p>
<p>Yet, to compute the mutual information PWS requires evaluating the
conditional trajectory probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}| \mathbfit{s})</annotation></semantics></math>,
which in turn requires a stochastic model defining a probability measure
over trajectories. While (stochastic) mechanistic models of experimental
systems are increasingly becoming available, the question remains
whether PWS can be applied directly to experimental data when no such
model is available. In <a href="#ch:ml-pws">Ch. ml-pws</a>, we show that
machine learning can be used to construct a data-driven stochastic model
that captures the trajectory statistics, i.e.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}| \mathbfit{s})</annotation></semantics></math>,
enabling the application of PWS to experimental data.</p>
<p>We demonstrate the practical utility of PWS by calculating the
trajectory mutual information for a range of systems. In <a
href="#ch:variants">Ch. variants</a> and <a href="#ch:lna_vs_pws">Ch.
lna_vs_pws</a>, we study a minimal model for gene expression, showing
that PWS can estimate the mutual information rate for this system more
accurately than any previous technique. Using PWS, we reveal that the
Gaussian approximation, though expected to hold due to the systemâ€™s
linearity, does not provide an accurate estimate in this case. In <a
href="#ch:lna_vs_pws">Ch. lna_vs_pws</a> and <a href="#ch:ml-pws">Ch.
ml-pws</a> we extend our analysis to simple nonlinear models for
information transmission, comparing PWS results against the Gaussian
approximation; for these models, PWS is the first technique capable of
accurately computing trajectory mutual information. Moreover, in <a
href="#ch:chemotaxis">Ch. chemotaxis</a> we apply PWS to a complex
stochastic model of bacterial chemotaxis, marking the first instance
where the information rate for a system of this complexity can be
computed exactly. Together, these examples demonstrate that an exact
technique like PWS is indispensable for understanding information
transmission in realistic scenarios.</p>
<h2 id="contributions-of-this-work">Contributions of This Work</h2>
<p>The main contributions of this thesis are as follows:</p>
<ol>
<li><p><strong>PWS: A novel framework for computing the trajectory
mutual information</strong>: We introduce Path Weight Sampling, a
computational framework for calculating the trajectory mutual
information in dynamical stochastic systems. This framework is exact,
applicable to both continuous and discrete time processes, and does not
rely on any assumptions about the systemâ€™s dynamics. PWS and its main
variants are described in <a href="#ch:dpws">Ch. dpws</a> and <a
href="#ch:variants">Ch. variants</a>.</p></li>
<li><p><strong>Discovery of discrepancies between experiments and
mathematical models of chemotaxis</strong>: We apply PWS to various
systems, including the complex bacterial chemotaxis signaling network.
By studying the information transmission rate of chemotaxis and
comparing our results against those of Mattingly et al. <span
class="citation" data-cites="2021.Mattingly">Â [<a
href="#ref-2021.Mattingly" role="doc-biblioref">4</a>]</span>, we find
that the widely-used MWC model of chemotaxis cannot explain the
experimental data. We find that the number of receptor clusters is
smaller and that the size of these clusters is larger than hitherto
believed. We describe and characterize this finding in <a
href="#ch:chemotaxis">Ch. chemotaxis</a>.</p></li>
<li><p><strong>Study of the accuracy of the gaussian approximation for
the information rate</strong>: In <a href="#ch:lna_vs_pws">Ch.
lna_vs_pws</a>, we use PWS to quantitatively study the accuracy of the
widely-used Gaussian approximation. Before PWS, no exact technique was
available to obtain <em>ground truth</em> results of the mutual
information rate for non-linear systems, and the accuracy of the
Gaussian framework could not be evaluated. We reveal that the Gaussian
model can be surprisingly inaccurate, even for linear reaction
systems.</p></li>
<li><p><strong>Neural networks for learning the stochastic dynamics from
time-series data</strong>: In <a href="#ch:ml-pws">Ch. ml-pws</a>, we
demonstrate that recent machine learning techniques can be employed to
automatically learn the stochastic dynamics from experimental data. We
show that by combining these learned models with PWS, it becomes
possible to compute the trajectory mutual information directly from
time-series data. This approach outperforms previous techniques, like
the Gaussian approximation, for estimating information rates from
data.</p></li>
</ol>
<h2 id="thesis-outline">Thesis Outline</h2>
<p>The remainder of this thesis is divided into 5 chapters. We first
present three variants of PWS, all of which compute the conditional
entropy in the same manner, but differ in the way this Monte Carlo
averaging procedure for computing the marginal probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’«</mi><mo stretchy="false" form="prefix">[</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{P}[\mathbfit{x}]</annotation></semantics></math>
is carried out. Chapters <a href="#ch:dpws">dpws</a>, <a
href="#ch:variants">variants</a> and <a
href="#ch:chemotaxis">chemotaxis</a> of this thesis have been published
previously in <em>Physical Review X</em>.<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>In <a href="#ch:dpws">Ch. dpws</a> we present the simplest PWS
variant, <em>Direct</em> PWS (DPWS). To compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’«</mi><mo stretchy="false" form="prefix">[</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{P}[\mathbfit{x}]</annotation></semantics></math>,
DPWS performs a brute-force average of the path likelihoods
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’«</mi><mo stretchy="false" form="prefix">[</mo><mi>ğ’™</mi><mo stretchy="false" form="prefix">|</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{P}[\mathbfit{x}|\mathbfit{s}]</annotation></semantics></math>
over the input trajectories
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’”</mi><annotation encoding="application/x-tex">\mathbfit{s}</annotation></semantics></math>.
While we show that this scheme works for simple systems, the brute-force
Monte Carlo averaging procedure becomes more difficult for larger
systems and exponentially harder for longer trajectories.</p>
<p>In <a href="#ch:variants">Ch. variants</a>, we present our second and
third variant of PWS which are based on the realization that the
marginal probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’«</mi><mo stretchy="false" form="prefix">[</mo><mi>ğ’™</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathcal{P}[\mathbfit{x}]</annotation></semantics></math>
is akin to a partition function. These schemes leverage techniques for
computing free energies from statistical physics. We also apply PWS to a
simple model system which consists of a simple pair of coupled
birth-death processes which allows us to compare the efficiency of the
three PWS variants, as well as to compare the PWS results against
analytical results from the Gaussian approximation <span
class="citation" data-cites="2009.Tostevin">Â [<a
href="#ref-2009.Tostevin" role="doc-biblioref">36</a>]</span>.</p>
<p>In <a href="#ch:chemotaxis">Ch. chemotaxis</a>, we apply PWS to the
bacterial chemotaxis system, which is arguably the best characterized
signaling system in biology. Mattingly et al. <span class="citation"
data-cites="2021.Mattingly">Â [<a href="#ref-2021.Mattingly"
role="doc-biblioref">4</a>]</span> recently argued that bacterial
chemotaxis in shallow gradients is information limited. Yet, to compute
the information rate from their experimental data they had to employ a
Gaussian framework. PWS makes it possible to asses the accuracy of this
approximation.</p>
<p><a href="#ch:lna_vs_pws">Chapter lna_vs_pws</a> is devoted to
studying the accuracy of the Gaussian approximation for non-Gaussian
systems. By understanding the limitations and strengths of the Gaussian
approximation, this chapter aims to provide deeper insights into
selecting the appropriate method for MI estimation depending on the
system.</p>
<p>Finally, <a href="#ch:ml-pws">Ch. ml-pws</a> we introduce ML-PWS,
which combines recent machine learning models with PWS, to compute the
mutual information directly from data. This idea significantly extends
the range of applications for PWS, since we no longer require a
mechanistic model of the system. Instead, the stochastic model is
automatically learned from the data.</p>
<h2 class="unnumbered" id="bibliography">References</h2>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-2004.Neumann" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">J.
V. Neumann and O. Morgenstern, <em><span class="nocase">Theory of games
and economic behavior</span></em>, 60th anniversary ed. (Princeton
University Press, Princeton, N.J., 2004).</div>
</div>
<div id="ref-1972.Savage" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">L.
J. Savage, <em><span class="nocase">The Foundations of
Statistics</span></em>, 2nd ed. (Dover Publications, New York,
1972).</div>
</div>
<div id="ref-2018.Schwarting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">W.
Schwarting, J. Alonso-Mora, and D. Rus, <a
href="https://doi.org/10.1146/annurev-control-060117-105157"><span
class="nocase">Planning and Decision-Making for Autonomous
Vehicles</span></a>, Annual Review of Control, Robotics, and Autonomous
Systems <strong>1</strong>, 1 (2018).</div>
</div>
<div id="ref-2021.Mattingly" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">H.
H. Mattingly, K. Kamino, B. B. Machta, and T. Emonet, <a
href="https://doi.org/10.1038/s41567-021-01380-3"><span
class="nocase">Escherichia coli chemotaxis is information
limited</span></a>, Nature Physics <strong>17</strong>, 1426
(2021).</div>
</div>
<div id="ref-2012.Adami" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">C.
Adami, <a href="https://doi.org/10.1111/j.1749-6632.2011.06422.x"><span
class="nocase">The use of information theory in evolutionary
biology</span></a>, Annals of the New York Academy of Sciences
<strong>1256</strong>, 49 (2012).</div>
</div>
<div id="ref-2022.Hledik" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">M.
HledÃ­k, N. Barton, and G. TkaÄik, <a
href="https://doi.org/10.1073/pnas.2123152119"><span
class="nocase">Accumulation and maintenance of information in
evolution</span></a>, Proceedings of the National Academy of Sciences
<strong>119</strong>, e2123152119 (2022).</div>
</div>
<div id="ref-1948.Shannon" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">C.
E. Shannon, <a
href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x"><span
class="nocase">A Mathematical Theory of Communication</span></a>, Bell
System Technical Journal <strong>27</strong>, 379 (1948).</div>
</div>
<div id="ref-1998.Strong" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">S.
P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek, <a
href="https://doi.org/10.1103/physrevlett.80.197"><span
class="nocase">Entropy and Information in Neural Spike
Trains</span></a>, Physical Review Letters <strong>80</strong>, 197
(1998).</div>
</div>
<div id="ref-1999.Rieke" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">F.
Rieke, D. Warland, R. de Ruyter van Steveninck, and W. Bialek, <em><span
class="nocase">Spikes: exploring the neural code</span></em> (MIT Press,
Cambridge, Massachusetts, 1999).</div>
</div>
<div id="ref-2008.Tkacik" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">G.
TkaÄik, C. G. Callan, and W. Bialek, <a
href="https://doi.org/10.1073/pnas.0806077105"><span
class="nocase">Information flow and optimization in transcriptional
regulation</span></a>, Proceedings of the National Academy of Sciences
<strong>105</strong>, 12265 (2008).</div>
</div>
<div id="ref-2009.Mehta" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">P.
Mehta, S. Goyal, T. Long, B. L. Bassler, and N. S. Wingreen, <a
href="https://doi.org/10.1038/msb.2009.79"><span
class="nocase">Information processing and signal integration in
bacterial quorum sensing</span></a>, Molecular Systems Biology
<strong>5</strong>, 325 (2009).</div>
</div>
<div id="ref-2011.Cheong" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">R.
Cheong, A. Rhee, C. J. Wang, I. Nemenman, and A. Levchenko, <a
href="https://doi.org/10.1126/science.1204553"><span
class="nocase">Information Transduction Capacity of Noisy Biochemical
Signaling Networks</span></a>, Science <strong>334</strong>, 354
(2011).</div>
</div>
<div id="ref-2000.Tishby" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">N.
Tishby, F. C. Pereira, and W. Bialek, <a
href="https://doi.org/10.48550/arxiv.physics/0004057"><span
class="nocase">The information bottleneck method</span></a>, arXiv
(2000).</div>
</div>
<div id="ref-2015.Tishby" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">N.
Tishby and N. Zaslavsky, <a
href="https://doi.org/10.48550/arxiv.1503.02406"><span
class="nocase">Deep Learning and the Information Bottleneck
Principle</span></a>, arXiv (2015).</div>
</div>
<div id="ref-2017.Schwartz-Ziv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">R.
Shwartz-Ziv and N. Tishby, <a
href="https://doi.org/10.48550/arxiv.1703.00810"><span
class="nocase">Opening the Black Box of Deep Neural Networks via
Information</span></a>, arXiv (2017).</div>
</div>
<div id="ref-2014.Fiedor" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">P.
Fiedor, <a href="https://doi.org/10.1103/physreve.89.052801"><span
class="nocase">Networks in financial markets based on the mutual
information rate</span></a>, Physical Review E <strong>89</strong>,
052801 (2014).</div>
</div>
<div id="ref-1956.Kelly" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">J.
L. Kelly, <a
href="https://doi.org/10.1002/j.1538-7305.1956.tb03809.x"><span
class="nocase">A New Interpretation of Information Rate</span></a>, Bell
System Technical Journal <strong>35</strong>, 917 (1956).</div>
</div>
<div id="ref-2014.Aspelmeyer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">M.
Aspelmeyer, T. J. Kippenberg, and F. Marquardt, <a
href="https://doi.org/10.1103/revmodphys.86.1391"><span
class="nocase">Cavity optomechanics</span></a>, Reviews of Modern
Physics <strong>86</strong>, 1391 (2014).</div>
</div>
<div id="ref-2022.Peters" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">K.
J. H. Peters and S. R. K. Rodriguez, <a
href="https://doi.org/10.1103/physrevlett.129.013901"><span
class="nocase">Exceptional Precision of a Nonlinear Optical Sensor at a
Square-Root Singularity</span></a>, Physical Review Letters
<strong>129</strong>, 013901 (2022).</div>
</div>
<div id="ref-2006.Cover" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">T.
M. Cover and J. A. Thomas, <em><span class="nocase">Elements of
Information Theory</span></em>, 2nd ed. (John Wiley &amp; Sons,
2006).</div>
</div>
<div id="ref-2014.Kinney" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">J.
B. Kinney and G. S. Atwal, <a
href="https://doi.org/10.1073/pnas.1309933111"><span
class="nocase">Equitability, mutual information, and the maximal
information coefficient</span></a>, Proceedings of the National Academy
of Sciences <strong>111</strong>, 3354 (2014).</div>
</div>
<div id="ref-2016.Pilkiewicz" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">K.
R. Pilkiewicz and M. L. Mayo, <a
href="https://doi.org/10.1103/physreve.94.032412"><span
class="nocase">Fluctuation sensitivity of a transcriptional signaling
cascade</span></a>, Physical Review E <strong>94</strong>, 032412
(2016).</div>
</div>
<div id="ref-2024.Fan" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">R.
Fan and A. Hilfinger, <a
href="https://doi.org/10.1103/physreve.110.034309"><span
class="nocase">Characterizing the nonmonotonic behavior of mutual
information along biochemical reaction cascades</span></a>, Physical
Review E <strong>110</strong>, 034309 (2024).</div>
</div>
<div id="ref-2024.Das" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">A.
Das and P. R. ten Wolde, <a
href="https://doi.org/10.48550/arxiv.2409.01650"><span
class="nocase">Exact computation of Transfer Entropy with Path Weight
Sampling</span></a>, arXiv (2024).</div>
</div>
<div id="ref-2009.Tkacik" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">G.
TkaÄik, A. M. Walczak, and W. Bialek, <a
href="https://doi.org/10.1103/physreve.80.031920"><span
class="nocase">Optimizing information flow in small genetic
networks</span></a>, Physical Review E <strong>80</strong>, 031920
(2009).</div>
</div>
<div id="ref-2010.Walczak" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">A.
M. Walczak, G. TkaÄik, and W. Bialek, <a
href="https://doi.org/10.1103/physreve.81.041905"><span
class="nocase">Optimizing information flow in small genetic networks.
II. Feed-forward interactions</span></a>, Physical Review E
<strong>81</strong>, 041905 (2010).</div>
</div>
<div id="ref-2012.Bowsher" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">C.
G. Bowsher and P. S. Swain, <a
href="https://doi.org/10.1073/pnas.1119407109"><span
class="nocase">Identifying sources of variation and the flow of
information in biochemical networks</span></a>, Proceedings of the
National Academy of Sciences <strong>109</strong>, E1320 (2012).</div>
</div>
<div id="ref-2014.Clausznitzer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">D.
Clausznitzer, G. Micali, S. Neumann, V. Sourjik, and R. G. Endres, <a
href="https://doi.org/10.1371/journal.pcbi.1003870"><span
class="nocase">Predicting Chemical Environments of Bacteria from
Receptor Signaling</span></a>, PLoS Computational Biology
<strong>10</strong>, e1003870 (2014).</div>
</div>
<div id="ref-2015.Palmer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">S.
E. Palmer, O. Marre, M. J. Berry, and W. Bialek, <a
href="https://doi.org/10.1073/pnas.1506855112"><span
class="nocase">Predictive information in a sensory
population</span></a>, Proceedings of the National Academy of Sciences
<strong>112</strong>, 6908 (2015).</div>
</div>
<div id="ref-1999.Borst" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">A.
Borst and F. E. Theunissen, <a
href="https://doi.org/10.1038/14731"><span class="nocase">Information
theory and neural coding</span></a>, Nature Neuroscience
<strong>2</strong>, 947 (1999).</div>
</div>
<div id="ref-2008.Boulware" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">M.
J. Boulware and J. S. Marchant, <a
href="https://doi.org/10.1016/j.cub.2008.07.018"><span
class="nocase">Timing in Cellular Ca2+ Signaling</span></a>, Current
Biology <strong>18</strong>, R769 (2008).</div>
</div>
<div id="ref-2018.Mitra" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">T.
Mitra, S. N. Menon, and S. Sinha, <a
href="https://doi.org/10.1038/s41598-018-31626-9"><span
class="nocase">Emergent memory in cell signaling: Persistent adaptive
dynamics in cascades can arise from the diversity of relaxation
time-scales</span></a>, Scientific Reports <strong>8</strong>, 13230
(2018).</div>
</div>
<div id="ref-2023.Nalecz-Jawecki" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">P.
NaÅ‚Ä™cz-Jawecki, P. A. Gagliardi, M. KochaÅ„czyk, C. Dessauges, O. Pertz,
and T. Lipniacki, <a
href="https://doi.org/10.1371/journal.pcbi.1011155"><span
class="nocase">The MAPK/ERK channel capacity exceeds 6
bit/hour</span></a>, PLOS Computational Biology <strong>19</strong>,
e1011155 (2023).</div>
</div>
<div id="ref-2014.Selimkhanov" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">J.
Selimkhanov, B. Taylor, J. Yao, A. Pilko, J. Albeck, A. Hoffmann, L.
Tsimring, and R. Wollman, <a
href="https://doi.org/10.1126/science.1254933"><span
class="nocase">Accurate information transmission through dynamic
biochemical signaling networks</span></a>, Science <strong>346</strong>,
1370 (2014).</div>
</div>
<div id="ref-2008.Staniek" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">M.
Staniek and K. Lehnertz, <a
href="https://doi.org/10.1103/physrevlett.100.158101"><span>Symbolic
Transfer Entropy</span></a>, Physical Review Letters
<strong>100</strong>, 158101 (2008).</div>
</div>
<div id="ref-2009.Tostevin" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">F.
Tostevin and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physrevlett.102.218101"><span
class="nocase">Mutual Information between Input and Output Trajectories
of Biochemical Networks</span></a>, Physical Review Letters
<strong>102</strong>, 218101 (2009).</div>
</div>
<div id="ref-2012.Runge" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">J.
Runge, J. Heitzig, V. Petoukhov, and J. Kurths, <a
href="https://doi.org/10.1103/physrevlett.108.258701"><span
class="nocase">Escaping the Curse of Dimensionality in Estimating
Multivariate Transfer Entropy</span></a>, Physical Review Letters
<strong>108</strong>, 258701 (2012).</div>
</div>
<div id="ref-2021.Meijers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">M.
Meijers, S. Ito, and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physreve.103.l010102"><span
class="nocase">Behavior of information flow near criticality</span></a>,
Physical Review E <strong>103</strong>, L010102 (2021).</div>
</div>
<div id="ref-2021.Tang" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">Y.
Tang, A. Adelaja, F. X.-F. Ye, E. Deeds, R. Wollman, and A. Hoffmann, <a
href="https://doi.org/10.1038/s41467-021-21562-0"><span
class="nocase">Quantifying information accumulation encoded in the
dynamics of biochemical signaling</span></a>, Nature Communications
<strong>12</strong>, 1272 (2021).</div>
</div>
<div id="ref-2023.Hahn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">L.
Hahn, A. M. Walczak, and T. Mora, <a
href="https://doi.org/10.1103/physrevlett.131.128401"><span
class="nocase">Dynamical Information Synergy in Biochemical Signaling
Networks</span></a>, Physical Review Letters <strong>131</strong>,
128401 (2023).</div>
</div>
<div id="ref-2024.Umeki" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">N.
Umeki, Y. Kabashima, and Y. Sako, <a
href="https://doi.org/10.1101/2023.08.06.552214"><span
class="nocase">Evaluation of information flows in the RAS-MAPK system
using transfer entropy measurements</span></a>, bioRxiv
2023.08.06.552214 (2024).</div>
</div>
<div id="ref-2024.Nicoletti" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">G.
Nicoletti and D. M. Busiello, <a
href="https://doi.org/10.1103/physrevlett.133.158401"><span
class="nocase">Tuning Transduction from Hidden Observables to Optimize
Information Harvesting</span></a>, Physical Review Letters
<strong>133</strong>, 158401 (2024).</div>
</div>
<div id="ref-2000.Schreiber" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">T.
Schreiber, <a href="https://doi.org/10.1103/physrevlett.85.461"><span
class="nocase">Measuring information transfer</span></a>, Physical
Review Letters <strong>85</strong>, 461 (2000).</div>
</div>
<div id="ref-2002.Kaiser" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">A.
Kaiser and T. Schreiber, <a
href="https://doi.org/10.1016/s0167-2789(02)00432-3"><span
class="nocase">Information transfer in continuous processes</span></a>,
Physica D: Nonlinear Phenomena <strong>166</strong>, 43 (2002).</div>
</div>
<div id="ref-2013.Barato" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">A.
C. Barato, D. Hartich, and U. Seifert, <a
href="https://doi.org/10.1007/s10955-013-0834-5"><span
class="nocase">Rate of Mutual Information Between Coarse-Grained
Non-Markovian Variables</span></a>, Journal of Statistical Physics
<strong>153</strong>, 460 (2013).</div>
</div>
<div id="ref-2014.Hartich" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline">D.
Hartich, A. C. Barato, and U. Seifert, <a
href="https://doi.org/10.1088/1742-5468/2014/02/p02016"><span
class="nocase">Stochastic thermodynamics of bipartite systems: transfer
entropy inequalities and a Maxwellâ€™s demon interpretation</span></a>,
Journal of Statistical Mechanics: Theory and Experiment
<strong>2014</strong>, P02016 (2014).</div>
</div>
<div id="ref-2003.Paninski" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline">L.
Paninski, <a href="https://doi.org/10.1162/089976603321780272"><span
class="nocase">Estimation of Entropy and Mutual Information</span></a>,
Neural Computation <strong>15</strong>, 1191 (2003).</div>
</div>
<div id="ref-2014.Tkacik" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline">G.
TkaÄik, J. O. Dubuis, M. D. Petkova, and T. Gregor, <a
href="https://doi.org/10.1534/genetics.114.171850"><span
class="nocase">Positional Information, Positional Error, and Read-Out
Precision in Morphogenesis: A Mathematical Framework</span></a>,
Genetics <strong>199</strong>, genetics.114.171850 (2014).</div>
</div>
<div id="ref-2004.Kraskov" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">A.
Kraskov, H. StÃ¶gbauer, and P. Grassberger, <a
href="https://doi.org/10.1103/physreve.69.066138"><span
class="nocase">Estimating mutual information</span></a>, Physical Review
E <strong>69</strong>, 066138 (2004).</div>
</div>
<div id="ref-2019.Cepeda-Humerez" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div class="csl-right-inline">S.
A. Cepeda-Humerez, J. Ruess, and G. TkaÄik, <a
href="https://doi.org/10.1371/journal.pcbi.1007290"><span
class="nocase">Estimating information in time-varying
signals.</span></a>, PLoS Computational Biology <strong>15</strong>,
e1007290 (2019).</div>
</div>
<div id="ref-1983.Donsker" class="csl-entry" role="listitem">
<div class="csl-left-margin">[51] </div><div class="csl-right-inline">M.
D. Donsker and S. R. S. Varadhan, <a
href="https://doi.org/10.1002/cpa.3160360204"><span
class="nocase">Asymptotic evaluation of certain markov process
expectations for large time. IV</span></a>, Communications on Pure and
Applied Mathematics <strong>36</strong>, 183 (1983).</div>
</div>
<div id="ref-2018.Belghazi" class="csl-entry" role="listitem">
<div class="csl-left-margin">[52] </div><div class="csl-right-inline">M.
I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, A.
Courville, and D. Hjelm, <em><a
href="https://proceedings.mlr.press/v80/belghazi18a.html">Mutual
Information Neural Estimation</a></em>, in <em>Proceedings of the 35th
International Conference on Machine Learning</em>, edited by J. Dy and
A. Krause, Vol. 80 (PMLR, 2018), pp. 531â€“540.</div>
</div>
<div id="ref-2018.McAllester" class="csl-entry" role="listitem">
<div class="csl-left-margin">[53] </div><div class="csl-right-inline">D.
McAllester and K. Stratos, <a
href="https://doi.org/10.48550/arxiv.1811.04251"><span
class="nocase">Formal Limitations on the Measurement of Mutual
Information</span></a>, arXiv (2018).</div>
</div>
<div id="ref-2005.Baronchelli" class="csl-entry" role="listitem">
<div class="csl-left-margin">[54] </div><div class="csl-right-inline">A.
Baronchelli, E. Caglioti, and V. Loreto, <a
href="https://doi.org/10.1088/0143-0807/26/5/s08"><span
class="nocase">Measuring complexity with zippers</span></a>, European
Journal of Physics <strong>26</strong>, S69 (2005).</div>
</div>
<div id="ref-2008.Gao" class="csl-entry" role="listitem">
<div class="csl-left-margin">[55] </div><div class="csl-right-inline">Y.
Gao, I. Kontoyiannis, and E. Bienenstock, <a
href="https://doi.org/10.3390/entropy-e10020071"><span
class="nocase">Estimating the Entropy of Binary Time Series:
Methodology, Some Theory and a Simulation Study</span></a>, Entropy
<strong>10</strong>, 71 (2008).</div>
</div>
<div id="ref-2023.Sinzger" class="csl-entry" role="listitem">
<div class="csl-left-margin">[56] </div><div class="csl-right-inline">M.
Sinzger-Dâ€™Angelo and H. Koeppl, <a
href="https://doi.org/10.1109/tit.2023.3293996"><span
class="nocase">Counting Processes with Piecewise-Deterministic Markov
Conditional Intensity: Asymptotic Analysis, Implementation and
Information-Theoretic Use</span></a>, IEEE Transactions on Information
Theory 1 (2023).</div>
</div>
<div id="ref-2024.Gehri" class="csl-entry" role="listitem">
<div class="csl-left-margin">[57] </div><div class="csl-right-inline">M.
Gehri, N. Engelmann, and H. Koeppl, <a
href="https://doi.org/10.48550/arxiv.2403.15221"><span
class="nocase">Mutual Information of a class of Poisson-type Channels
using Markov Renewal Theory</span></a>, arXiv (2024).</div>
</div>
<div id="ref-2019.Duso" class="csl-entry" role="listitem">
<div class="csl-left-margin">[58] </div><div class="csl-right-inline">L.
Duso and C. Zechner, <em><a
href="https://doi.org/10.1109/CDC40024.2019.9029316">Path Mutual
Information for a Class of Biochemical Reaction Networks</a></em>, in
<em>2019 IEEE 58th Conference on Decision and Control (CDC)</em> (2019),
pp. 6610â€“6615.</div>
</div>
<div id="ref-2023.Moor" class="csl-entry" role="listitem">
<div class="csl-left-margin">[59] </div><div
class="csl-right-inline">A.-L. Moor and C. Zechner, <a
href="https://doi.org/10.1103/physrevresearch.5.013032"><span
class="nocase">Dynamic information transfer in stochastic biochemical
networks</span></a>, Physical Review Research <strong>5</strong>, 013032
(2023).</div>
</div>
<div id="ref-1955.Rosenbluth" class="csl-entry" role="listitem">
<div class="csl-left-margin">[60] </div><div class="csl-right-inline">M.
N. Rosenbluth and A. W. Rosenbluth, <a
href="https://doi.org/10.1063/1.1741967"><span class="nocase">Monte
Carlo Calculation of the Average Extension of Molecular
Chains</span></a>, The Journal of Chemical Physics <strong>23</strong>,
356 (1955).</div>
</div>
<div id="ref-1990.Siepmann" class="csl-entry" role="listitem">
<div class="csl-left-margin">[61] </div><div class="csl-right-inline">J.
I. Siepmann, <a href="https://doi.org/10.1080/00268979000101591"><span
class="nocase">A method for the direct calculation of chemical
potentials for dense chain systems</span></a>, Molecular Physics
<strong>70</strong>, 1145 (1990).</div>
</div>
<div id="ref-1997.Grassberger" class="csl-entry" role="listitem">
<div class="csl-left-margin">[62] </div><div class="csl-right-inline">P.
Grassberger, <a href="https://doi.org/10.1103/physreve.56.3682"><span
class="nocase">Pruned-enriched Rosenbluth method: Simulations of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î¸</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
polymers of chain length up to 1 000 000</span></a>, Physical Review E
<strong>56</strong>, 3682 (1997).</div>
</div>
<div id="ref-2002.Frenkel" class="csl-entry" role="listitem">
<div class="csl-left-margin">[63] </div><div class="csl-right-inline">D.
Frenkel and B. Smit, <em><a
href="https://doi.org/10.1016/b978-0-12-267351-1.x5000-7"><span>Understanding
Molecular Simulation</span></a></em>, 2nd ed. (Academic Press, San
Diego, 2002).</div>
</div>
<div id="ref-2002.Bolhuis" class="csl-entry" role="listitem">
<div class="csl-left-margin">[64] </div><div class="csl-right-inline">P.
G. Bolhuis, D. Chandler, C. Dellago, and P. L. Geissler, <a
href="https://doi.org/10.1146/annurev.physchem.53.082301.113146"><span
class="nocase">TRANSITION PATH SAMPLING: Throwing Ropes Over Rough
Mountain Passes, in the Dark</span></a>, Annual Review of Physical
Chemistry <strong>53</strong>, 291 (2002).</div>
</div>
<div id="ref-1998.Gelman" class="csl-entry" role="listitem">
<div class="csl-left-margin">[65] </div><div class="csl-right-inline">A.
Gelman and X.-L. Meng, <a
href="https://doi.org/10.1214/ss/1028905934"><span
class="nocase">Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling</span></a>, Statistical
Science <strong>13</strong>, 163 (1998).</div>
</div>
<div id="ref-2001.Neal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[66] </div><div class="csl-right-inline">R.
M. Neal, <a href="https://doi.org/10.1023/a:1008923215028"><span
class="nocase">Annealed importance sampling</span></a>, Statistics and
Computing <strong>11</strong>, 125 (2001).</div>
</div>
<div id="ref-2013.Kingma" class="csl-entry" role="listitem">
<div class="csl-left-margin">[67] </div><div class="csl-right-inline">D.
P. Kingma and M. Welling, <a
href="https://doi.org/10.48550/arxiv.1312.6114"><span>Auto-Encoding
Variational Bayes</span></a>, arXiv (2013).</div>
</div>
<div id="ref-2023.Reinhardt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[68] </div><div class="csl-right-inline">M.
Reinhardt, G. TkaÄik, and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physrevx.13.041017"><span
class="nocase">Path Weight Sampling: Exact Monte Carlo Computation of
the Mutual Information between Stochastic Trajectories</span></a>,
Physical Review X <strong>13</strong>, 041017 (2023).</div>
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>In mathematical terms, this interplay between
information and reward can be characterized by utility functions, which
quantify the benefits of different actions based on available
information <span class="citation"
data-cites="2004.Neumann 1972.Savage">Â [<a href="#ref-2004.Neumann"
role="doc-biblioref">1</a>,<a href="#ref-1972.Savage"
role="doc-biblioref">2</a>]</span>.<a href="#fnref1"
class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn2"><p>In contrast to other correlation measures used in
statistics, such as the Pearson correlation coefficient, the mutual
information captures both linear and nonlinear dependencies between
variables. Additionally, in contrast to other correlation measures, the
mutual information satisfies the data processing inequality, which
states that no type of post-processing can increase the mutual
information between the input and output <span class="citation"
data-cites="2006.Cover 2014.Kinney">Â [<a href="#ref-2006.Cover"
role="doc-biblioref">20</a>,<a href="#ref-2014.Kinney"
role="doc-biblioref">21</a>]</span>. These properties make the mutual
information uniquely suited for describing the fidelity of the
input-output mapping in information-processing systems. Note however
that a naÃ¯ve use of the data processing inequality leads to seemingly
contradictory results when applied to the stationary dynamics of
processing cascades <span class="citation"
data-cites="2016.Pilkiewicz 2024.Fan 2024.Das">Â [<a
href="#ref-2016.Pilkiewicz" role="doc-biblioref">22</a>â€“<a
href="#ref-2024.Das" role="doc-biblioref">24</a>]</span>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn3"><p>M. Reinhardt, G. TkaÄik, and P. R. ten Wolde, Path
Weight Sampling: Exact Monte Carlo Computation of the Mutual Information
between Stochastic Trajectories, <em>Phys. Rev. X</em>
<strong>13</strong>, 041017 (2023) <span class="citation"
data-cites="2023.Reinhardt">Â [<a href="#ref-2023.Reinhardt"
role="doc-biblioref">68</a>]</span><a href="#fnref3"
class="footnote-back" role="doc-backlink">â†©ï¸</a></p></li>
</ol>
</section>
</body>
</html>
