<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>LNA vs PWS</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../css/thesis-style.css" />
  <a href="../index.html" class="back-link">â† Back to Contents</a>
  <script src="../js/equation-tooltips.js"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">LNA vs PWS</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#ch:lna_vs_pws" id="toc-ch:lna_vs_pws">The Accuracy of the
Gaussian Approximation</a>
<ul>
<li><a href="#methods" id="toc-methods">Methods</a>
<ul>
<li><a href="#the-mutual-information-rate"
id="toc-the-mutual-information-rate">The mutual information
rate</a></li>
<li><a href="#gaussian-approximation"
id="toc-gaussian-approximation">Gaussian Approximation</a></li>
<li><a href="#path-weight-sampling-for-diffusive-systems"
id="toc-path-weight-sampling-for-diffusive-systems">Path Weight Sampling
for diffusive systems</a></li>
</ul></li>
<li><a href="#case-studies" id="toc-case-studies">Case Studies</a>
<ul>
<li><a href="#sec:linsys" id="toc-sec:linsys">Discrete reaction
system</a></li>
<li><a href="#sec:nonlinsys" id="toc-sec:nonlinsys">Nonlinear continuous
system</a></li>
</ul></li>
<li><a href="#discussion" id="toc-discussion">Discussion</a></li>
<li><a href="#supplementary-information"
id="toc-supplementary-information">Supplementary Information</a>
<ul>
<li><a href="#app:gauss" id="toc-app:gauss">Gaussian
approximation</a></li>
<li><a href="#sec:relative-deviation"
id="toc-sec:relative-deviation">Relative deviation of the Gaussian
approximation for a nonlinear system</a></li>
</ul></li>
</ul></li>
<li><a href="#bibliography" id="toc-bibliography">References</a></li>
</ul>
</nav>
<h1 id="abstract">Abstract</h1>
<blockquote>
<p>Efficient information processing is crucial for both living organisms
and engineered systems. The mutual information rate, a core concept of
information theory, quantifies the amount of information shared between
the trajectories of input and output signals, and allows to
quantification of information flow in dynamic systems. A common approach
for estimating the mutual information rate is the Gaussian
approximation, which assumes that the input and output trajectories
follow Gaussian statistics. However, this method is limited to linear
systems, and its accuracy in nonlinear or discrete systems remains
unclear. In this work, we assess the accuracy of the Gaussian
approximation for non-Gaussian systems by leveraging Path Weight
Sampling (PWS), a recent technique for exactly computing the mutual
information rate. In two case studies, we examine the limitations of the
Gaussian approximation. First, we focus on discrete linear systems and
demonstrate that, even when the systemâ€™s statistics are nearly Gaussian,
the Gaussian approximation fails to accurately estimate the mutual
information rate. Second, we explore a continuous diffusive system with
a nonlinear transfer function, revealing significant deviations between
the Gaussian approximation and the exact mutual information rate as
nonlinearity increases. Our results provide a quantitative evaluation of
the Gaussian approximationâ€™s performance across different stochastic
models and highlight when more computationally intensive methods, such
as PWS, are necessary.</p>
</blockquote>
<h1 id="ch:lna_vs_pws">The Accuracy of the Gaussian Approximation</h1>
<p>For the functioning of both living and engineered systems it is
paramount that they collect and process information effectively.
Increasingly, it has become evident that beyond instantaneous
properties, the dynamic features of an input signal or system output
often encode valuable information <span class="citation"
data-cites="2009.Tostevin 2010.Tostevin 2021.Mattingly 2023.Moor 2023.Reinhardt 2023.Hahn">Â [<a
href="#ref-2009.Tostevin" role="doc-biblioref">1</a>â€“<a
href="#ref-2023.Hahn" role="doc-biblioref">6</a>]</span>. Prime examples
in biology include bacterial chemotaxis, which responds to temporal
changes in concentration <span class="citation"
data-cites="1986.Segall">Â [<a href="#ref-1986.Segall"
role="doc-biblioref">7</a>]</span>, the transcription factor
NF-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>B,
which encodes information about input signals in its dynamic response
<span class="citation" data-cites="covert2005achieving">Â [<a
href="#ref-covert2005achieving" role="doc-biblioref">8</a>]</span>, and
neuronal information processing, where information is encoded in the
sequence and timing of spikes <span class="citation"
data-cites="1998.Strong">Â [<a href="#ref-1998.Strong"
role="doc-biblioref">9</a>]</span>. Beyond biology, dynamic input
signals are critical for various sensing systems, such as those used in
automated factories or self-driving cars.</p>
<p>To understand and evaluate the performance, potential improvements,
and limitations of these systems in processing information, we need
appropriate metrics that capture their full information processing
capability. Information theory, introduced by Shannon <span
class="citation" data-cites="1948.Shannon">Â [<a href="#ref-1948.Shannon"
role="doc-biblioref">10</a>]</span>, provides the most general
mathematical framework for such metrics. The mutual information and
mutual information rate measure how much one random variable reduces
uncertainty about another, quantified in bits. It is relatively
straightforward to quantify the information shared between scalar
properties of the input and output, as has been done in various forms
<span class="citation"
data-cites="2007.Ziv 2011.Cheong dubuis2013positional 2015.Palmer chalk2018toward bauer2021trading sachdeva2021optimal tjalma2023trade tjalma2024predicting">Â [<a
href="#ref-2007.Ziv" role="doc-biblioref">11</a>â€“<a
href="#ref-tjalma2024predicting" role="doc-biblioref">19</a>]</span>.
However, capturing all information in dynamical properties of the input
and the output, is much more challenging. To do so, one must consider
the information encoded time-varying trajectories of the variables of
interest. Yet, due to the high dimensionality of the trajectory space,
computing the mutual information between such trajectories is
notoriously difficult.</p>
<p>A major advancement in this area has been the Gaussian approximation
of the mutual information rate <span class="citation"
data-cites="2009.Tostevin 2010.Tostevin">Â [<a href="#ref-2009.Tostevin"
role="doc-biblioref">1</a>,<a href="#ref-2010.Tostevin"
role="doc-biblioref">2</a>]</span>, based on the assumption of input and
output trajectories following jointly Gaussian statistics. This
assumption makes it possible to compute the mutual information rate
directly from the two-point correlation functions of the input and
output. It is thus straightforward to apply the Gaussian approximation
to experimental data. Moreover, given a mechanistic model of the
underlying dynamics, the Gaussian approximation can be used to derive
analytical expressions for the information rate <span class="citation"
data-cites="2009.Tostevin 2010.Tostevin 2021.Mattingly">Â [<a
href="#ref-2009.Tostevin" role="doc-biblioref">1</a>â€“<a
href="#ref-2021.Mattingly" role="doc-biblioref">3</a>]</span>. Crucially
however, the assumption of Gaussian statistics restricts the method to
linear systems, as Gaussian statistics can only arise in such systems
<span class="citation" data-cites="2006.Cover">Â [<a
href="#ref-2006.Cover" role="doc-biblioref">20</a>]</span>.</p>
<p>Understanding when the Gaussian approximation is accurate is critical
because many real-world systems, such as biological and engineered
sensory systems, exhibit nonlinear dynamics. This includes features such
as bimodality, discrete jumps, or heavy tails, all of which deviate from
purely Gaussian dynamics. Such non-Gaussian behavior typically results
from intrinsic nonlinearities in the system, but determining the degree
of a systemâ€™s deviation from linearity is difficult <span
class="citation"
data-cites="2008.Tkacik deronde2010effect mitra2001nonlinear">Â [<a
href="#ref-2008.Tkacik" role="doc-biblioref">21</a>â€“<a
href="#ref-mitra2001nonlinear" role="doc-biblioref">23</a>]</span>, and
the extent to which the approximation loses accuracy in nonlinear
systems is unclear. Thus, although the Gaussian approximation offers a
computationally simple framework to estimate information transmission,
it remains an open question under what conditions this approximation is
sufficiently accurate.</p>
<p>Until recently, addressing this question has been hard because there
was no reliable benchmark for the exact information rate. Without a
method to compute the true information rate of a non-Gaussian system, it
is impossible to rigorously assess the accuracy of the Gaussian
approximation. This gap was filled by the development of two independent
methods <span class="citation"
data-cites="2023.Reinhardt 2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>,<a href="#ref-2023.Reinhardt"
role="doc-biblioref">5</a>]</span> for computing the information rate
accurately even in systems that significantly deviate from Gaussian
behavior. Here we leverage one of these methods: Path Weight Sampling
(PWS) <span class="citation" data-cites="2023.Reinhardt">Â [<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span>. This is a
Monte Carlo technique which is an exact method for calculating the
mutual information rate in a wide range of stochastic models.</p>
<p>Using PWS, we can directly evaluate the accuracy of the Gaussian
approximation in models that exhibit explicit non-Gaussian features, and
study the approximationâ€™s robustness in typical applications.</p>
<p>In this article, we investigate the accuracy of the approximate
Gaussian information rate through two case studies. The first focuses on
Markov jump processes, where the statistics are non-Gaussian due to the
discrete nature of the processes. Perhaps surprisingly, the Gaussian
approximation fails to accurately estimate the mutual information rate
in this case, even when the statistics are nearly Gaussian <span
class="citation" data-cites="2023.Moor 2023.Reinhardt">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>,<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span>. We show
that a recently developed reaction-based â€œdiscrete approximationâ€ by
Moor and Zechner <span class="citation" data-cites="2023.Moor">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>]</span> is much more
accurate. This suggests that the Gaussian approximation fails because it
cannot distinguish the individual reaction events.</p>
<p>The second case study examines a continuous diffusive process with a
nonlinear transfer function. We demonstrate how intrinsic nonlinearity
can cause significant deviations between the Gaussian approximation and
the true mutual information rate. By varying the degree of nonlinearity
as well as the systemâ€™s response timescale, we provide a comprehensive
quantitative understanding of the Gaussian approximationâ€™s limitations
in nonlinear systems. Additionally, we show that for such systems, the
Gaussian approximation differs significantly when derived from empirical
correlation functions compared to when it is analytically obtained from
the nonlinear model, highlighting that the correct application of the
approximation is important.</p>
<p>Our work translates into concrete recommendations on when to use
which method for the computation of the information rate. It therefore
enables researchers to more confidently determine when a simpler
approximate method is sufficient, or when a more sophisticated method
like PWS <span class="citation" data-cites="2023.Reinhardt">Â [<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span> or the
method developed by Moor and Zechner <span class="citation"
data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span> should be used.</p>
<h2 id="methods">Methods</h2>
<h3 id="the-mutual-information-rate">The mutual information rate</h3>
<p>The mutual information between two random variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is defined as </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>âˆ¬</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mrow><mfrac><mi mathvariant="normal">P</mi><mi>(</mi></mfrac><mi>s</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><mspace width="0.222em"></mspace><mi>d</mi><mi>s</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">I(S, X) = \iint \mathrm{P}(s, x) \ln{\frac{\mathrm{P}}(s, x)}{\mathrm{P}(s)\mathrm{P}(x)}\ ds\,dx \,,</annotation></semantics></math>
<p> or, equivalently, using Shannon entropies </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="prefix">|</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo stretchy="false" form="prefix">|</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
I(S, X) 
&amp;= H(S) + H(X) - H(S, X) \\
&amp;= H(S) - H(S|X) \\
&amp;= H(X) - H(X|S)\,.
\end{aligned}</annotation></semantics></math>
<p> In the context of a noisy communication channel,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
represent the messages at the sending and receiving end, respectively.
Then,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S, X)</annotation></semantics></math>
is the amount of information about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
that is communicated when only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is received. If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
can be perfectly reconstructed from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S,X)=H(S)</annotation></semantics></math>.
On the contrary, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are independent,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">I(S, X)=0</annotation></semantics></math>.
The mutual information thus is always non-negative and quantifies the
degree of statistical dependence between two random variables.</p>
<p>For systems that continuously transmit information over time, this
concept must be extended to trajectories
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ£</mo><mi>t</mi><mo>âˆˆ</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbfit{S}_T=\{S(t) \mid t \in [0, T]\}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ£</mo><mi>t</mi><mo>âˆˆ</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbfit{X}_T=\{X(t) \mid t \in [0, T]\}</annotation></semantics></math>.
The mutual information between trajectories is defined analogously as
</p>
<div id="eq:trajectory_mi">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">âŸ¨</mo><mi>ln</mi><mrow><mfrac><mi mathvariant="normal">P</mi><mi>(</mi></mfrac><msub><mi>ğ’”</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">âŸ©</mo></mrow></mrow><annotation encoding="application/x-tex">I(\mathbfit{S}_T,\mathbfit{X}_T) = \left\langle \ln{\frac{\mathrm{P}}(\mathbfit{s}_T, \mathbfit{x}_T)}{\mathrm{P}(\mathbfit{s}_T) \mathrm{P}(\mathbfit{x}_T)} \right\rangle 
\label{eq:trajectory_mi}</annotation></semantics></math>
</div>
<p> where the expected value is taken with respect to the full joint
probability of both trajectories. This quantity can be interpreted as
the total information that is communicated over the time interval
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, T]</annotation></semantics></math>.</p>
<p>Note that the total amount of information communicated over the
time-interval
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0, T]</annotation></semantics></math>
is not directly related to the instantaneous mutual information
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S(t), X(t))</annotation></semantics></math>
at any instant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>âˆˆ</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">t \in [0,T]</annotation></semantics></math>.
This is because auto-correlations within the input or output sequences
reduce the amount of new information transmitted in subsequent
measurements. Moreover, information can be encoded in temporal features
of the trajectories, which cannot be captured by an instantaneous
information measure. Therefore, as previously pointed out <span
class="citation" data-cites="2021.Meijers 2024.Fan">Â [<a
href="#ref-2021.Meijers" role="doc-biblioref">24</a>,<a
href="#ref-2024.Fan" role="doc-biblioref">25</a>]</span>, the
instantaneous mutual information
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(S(t), X(t))</annotation></semantics></math>
for any given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>
does not provide a meaningful measure of information transmission. To
correctly quantify the amount of information transmitted per unit time
we must consider entire trajectories.</p>
<p>For that reason, the <em>mutual information rate</em> is defined via
the trajectory mutual information. Let the input and output of a system
be given by two continuous-time stochastic processes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’®</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ£</mo><mi>t</mi><mo>âˆˆ</mo><mi>â„</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{S}=\{S(t)\mid t\in\mathbb{R}\}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’³</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ£</mo><mi>t</mi><mo>âˆˆ</mo><mi>â„</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{X}=\{X(t)\mid t\in\mathbb{R}\}</annotation></semantics></math>.
Then, the mutual information rate between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’®</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math>
is </p>
<div id="eq:ratedef">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’®</mi><mo>,</mo><mi>ğ’³</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mi>lim</mi><mrow><mi>T</mi><mo>â†’</mo><mi>âˆ</mi></mrow></munder><mfrac><mn>1</mn><mi>T</mi></mfrac><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:ratedef}
R(\mathcal{S}, \mathcal{X}) = \lim_{T\to\infty} \frac{1}{T} I(\mathbfit{S}_T, \mathbfit{X}_T)\,,</annotation></semantics></math>
</div>
<p> and quantifies the amount of information that can reliably be
transmitted per unit time. The mutual information rate therefore
represents an excellent performance measure for information processing
systems.</p>
<p>In summary, the mutual information rate is <em>the</em> crucial
performance metric for stochastic information processing systems.
However, its information-theoretic definition does not translate into an
obvious scheme for computing it. As a result, various methods have been
developed to compute or approximate the mutual information rate.</p>
<h3 id="gaussian-approximation">Gaussian Approximation</h3>
<p>One way to significantly simplify the computation of the information
rate, is to assume that the input and output trajectories obey
stationary Gaussian statistics. Under this assumption <a
href="#eq:trajectory_mi">eq:trajectory_mi</a> simplifies to, </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mrow><mfrac><mrow><mo stretchy="false" form="prefix">|</mo><mi>ğ‘ª</mi></mrow><mi>_</mi></mfrac><mrow><mi>s</mi><mi>s</mi></mrow><mo stretchy="false" form="prefix">|</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>ğ‘ª</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo></mrow><mrow><mo stretchy="false" form="prefix">|</mo><mi>ğ’</mi><mo stretchy="false" form="prefix">|</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">I(\mathbfit{S}_T,\mathbfit{X}_T) = \frac{1}{2}\ln{\frac{|\mathbfit{C}}_{ss}||\mathbfit{C}_{xx}|}{|\mathbfit{Z}|},</annotation></semantics></math>
<p> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>ğ‘ª</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo></mrow><annotation encoding="application/x-tex">|\mathbfit{C}_{ss}|</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>ğ‘ª</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">|</mo></mrow><annotation encoding="application/x-tex">|\mathbfit{C}_{xx}|</annotation></semantics></math>
are the determinants of the covariance matrices of the respective
trajectories
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mrow><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo></mrow></msub><annotation encoding="application/x-tex">S_{[0,T]}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mrow><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="false" form="postfix">]</mo></mrow></msub><annotation encoding="application/x-tex">X_{[0,T]}</annotation></semantics></math>,
and </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>ğ‘ª</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>ğ‘ª</mi><mrow><mi>s</mi><mi>x</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>ğ‘ª</mi><mrow><mi>x</mi><mi>s</mi></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>ğ‘ª</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbfit{Z} = \begin{pmatrix}
        \mathbfit{C}_{ss} &amp; \mathbfit{C}_{sx} \\
        \mathbfit{C}_{xs} &amp; \mathbfit{C}_{xx}
    \end{pmatrix}</annotation></semantics></math>
<p> is the covariance matrix of their joint distribution.</p>
<p>In the limit that the trajectory length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mi>T</mi><mi>/</mi><mi mathvariant="normal">Î”</mi></mrow><annotation encoding="application/x-tex">N=T/\Delta</annotation></semantics></math>,
with the discretization
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Î”</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math>,
becomes infinitely long
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>â†’</mo><mi>âˆ</mi></mrow><annotation encoding="application/x-tex">N\to\infty</annotation></semantics></math>)
and continuous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo>â†’</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\Delta \to 0</annotation></semantics></math>),
the information rate as defined in <a href="#eq:ratedef">eq:ratedef</a>
can be expressed in terms of the power spectral densities, or power
spectra, of the processes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’®</mi><annotation encoding="application/x-tex">\mathcal S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal X</annotation></semantics></math>
<span class="citation" data-cites="2009.Tostevin 2010.Tostevin">Â [<a
href="#ref-2009.Tostevin" role="doc-biblioref">1</a>,<a
href="#ref-2010.Tostevin" role="doc-biblioref">2</a>]</span>: </p>
<div id="eq:gaussdef">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’®</mi><mo>,</mo><mi>ğ’³</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><mfrac><mn>1</mn><mrow><mn>4</mn><mi>Ï€</mi></mrow></mfrac><msubsup><mo>âˆ«</mo><mrow><mi>âˆ’</mi><mi>âˆ</mi></mrow><mi>âˆ</mi></msubsup><mi>d</mi><mi>Ï‰</mi><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mfrac><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>S</mi><mrow><mi>s</mi><mi>x</mi></mrow></msub><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><mrow><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><msub><mi>S</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\label{eq:gaussdef}
    R(\mathcal{S}, \mathcal{X}) = -\frac{1}{4\pi}\int_{-\infty}^{\infty} d\omega \ln{\left(1-\frac{|S_{sx}|^2}{S_{ss}S_{xx}}\right)}.</annotation></semantics></math>
</div>
<p> Here,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><annotation encoding="application/x-tex">S_{ss}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">S_{xx}</annotation></semantics></math>
respectively are the power spectra of trajectories generated by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’®</mi><annotation encoding="application/x-tex">\mathcal S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal X</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mrow><mi>s</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">S_{sx}</annotation></semantics></math>
is their cross-spectrum. The fraction </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ï•</mi><mrow><mi>s</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><msub><mi>S</mi><mrow><mi>s</mi><mi>x</mi></mrow></msub><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><mrow><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><msub><mi>S</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\phi_{sx}(\omega) = \frac{S_{sx}|^2}{S_{ss}S_{xx}}</annotation></semantics></math>
<p> is known as the coherence, describing the distribution of power
transfer between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’®</mi><annotation encoding="application/x-tex">\mathcal S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal X</annotation></semantics></math>
over the frequency
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ï‰</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>.</p>
<p>For systems that are neither Gaussian nor linear, there are two ways
to still obtain an approximate Gaussian information rate. The first is
to directly measure two-point correlation functions from data or
simulations, and use these to retrieve the power spectra in <a
href="#eq:gaussdef">eq:gaussdef</a>. The second is to use Kampen <span
class="citation" data-cites="2007.vanKampen">Â [<a
href="#ref-2007.vanKampen" role="doc-biblioref">26</a>]</span>â€™s linear
noise approximation (LNA) and approximate the dynamics of the system to
first order around a fixed point <span class="citation"
data-cites="2007.vanKampen">Â [<a href="#ref-2007.vanKampen"
role="doc-biblioref">26</a>]</span>, see also <a
href="#app:gauss">app:gauss</a>. In this work, we will analyze both of
these methods.</p>
<h3 id="path-weight-sampling-for-diffusive-systems">Path Weight Sampling
for diffusive systems</h3>
<p>To evaluate the accuracy of the Gaussian information rate for
non-Gaussian systems, an exact method for determining the true
information rate is required. Recently, a method called Path Weight
Sampling (PWS) was developed, which computes the exact mutual
information rate using Monte Carlo techniques without relying on
approximations <span class="citation" data-cites="2023.Reinhardt">Â [<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span>.</p>
<p>In Ref.Â <span class="citation" data-cites="2023.Reinhardt">Â [<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span>, PWS was
introduced as a computational framework for calculating the mutual
information rate in systems governed by master equations. Master
equations provide an exact stochastic description of continuous-time
processes with discrete state-spaces, commonly used in models ranging
from biochemical signaling networks to population dynamics. However,
many systems are not described by discrete state spaces and instead
require a stochastic description based on diffusion processes or other
stochastic models. Fortunately, PWS is not restricted to systems
described by master equations and can be extended to a variety of
stochastic models.</p>
<p>In general, PWS can be applied to any system that meets the following
conditions: (i)Â sampling from the input distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{s}_T)</annotation></semantics></math>
is straightforward, (ii)Â sampling from the conditional output
distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo>âˆ£</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}_T \mid \mathbfit{s}_T)</annotation></semantics></math>
is straightforward, and (iii)Â the logarithm of the conditional
probability density
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo>âˆ£</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\ln{\mathrm{P}}(\mathbfit{x}_T \mid \mathbfit{s}_T)</annotation></semantics></math>,
referred to as the path weight, can be evaluated efficiently. For any
stochastic model that satisfies these three criteria, the PWS
computation proceeds similarly to systems governed by master
equations.</p>
<p>Briefly, PWS computes the trajectory Mutual Information using a Monte
Carlo estimate of <a href="#eq:trajectory_mi">eq:trajectory_mi</a> </p>
<div id="eq:pws_monte_carlo">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mo stretchy="true" form="prefix">[</mo><mi>ln</mi><mi mathvariant="normal">P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>ğ’™</mi><mi>T</mi><mi>i</mi></msubsup><mo stretchy="true" form="infix">|</mo><msubsup><mi>ğ’”</mi><mi>T</mi><mi>i</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>ln</mi><mi mathvariant="normal">P</mi><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>ğ’™</mi><mi>T</mi><mi>i</mi></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mi>N</mi></mfrac><annotation encoding="application/x-tex">\frac{\sum^N_{i=1}\left[ \ln{\mathrm{P}}\left(\mathbfit{x}^i_T \middle| \mathbfit{s}^i_T\right) - \ln{\mathrm{P}}\left(\mathbfit{x}^i_T\right) \right] }{N}
    \label{eq:pws_monte_carlo}</annotation></semantics></math>
</div>
<p> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>ğ’”</mi><mi>T</mi><mn>1</mn></msubsup><mo>,</mo><mi>â€¦</mi><mo>,</mo><msubsup><mi>ğ’”</mi><mi>T</mi><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbfit{s}^1_T, \ldots, \mathbfit{s}^N_T</annotation></semantics></math>
are independently drawn from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{s}_T)</annotation></semantics></math>,
and each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>ğ’™</mi><mi>T</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">\mathbfit{x}^i_T</annotation></semantics></math>
is drawn from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo>âˆ£</mo><msubsup><mi>ğ’”</mi><mi>T</mi><mi>i</mi></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}_T \mid \mathbfit{s}^i_T)</annotation></semantics></math>.
As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>â†’</mo><mi>âˆ</mi></mrow><annotation encoding="application/x-tex">N\to\infty</annotation></semantics></math>,
this expression converges to the mutual information
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ‘º</mi><mi>T</mi></msub><mo>,</mo><msub><mi>ğ‘¿</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I(\mathbfit{S}_T,\mathbfit{X}_T)</annotation></semantics></math>.
In <a href="#eq:pws_monte_carlo">eq:pws_monte_carlo</a>, the term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo>âˆ£</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\ln{\mathrm{P}}(\mathbfit{x}_T \mid \mathbfit{s}_T)</annotation></semantics></math>
can be evaluated directly (per criterion iii), but the marginal
probability
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}_T)</annotation></semantics></math>
has to computed in separately for each output trajectory
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>ğ’™</mi><mi>T</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">\mathbfit{x}^i_T</annotation></semantics></math>.
Typically, this has to be done numerically via marginalization, i.e., by
computing the path integral </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>âˆ«</mo><mi>d</mi><msub><mi>ğ’”</mi><mi>T</mi></msub><mspace width="0.222em"></mspace><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>ğ’™</mi><mi>T</mi></msub><mo>âˆ£</mo><msub><mi>ğ’”</mi><mi>T</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{P}(\mathbfit{x}_T) = \int d\mathbfit{s}_T\ \mathrm{P}(\mathbfit{s}_T) 
    \mathrm{P}(\mathbfit{x}_T \mid \mathbfit{s}_T)</annotation></semantics></math>
<p> using Monte Carlo techniques. Evaluating the marginalization
integral efficiently is essential for computing the mutual information
using PWS and discussed in detail in Ref.Â <span class="citation"
data-cites="2023.Reinhardt">Â [<a href="#ref-2023.Reinhardt"
role="doc-biblioref">5</a>]</span>. In summary, PWS is a generic
framework that can be used beyond systems defined by a master equation
as long as a suitable generative model satisfying the three conditions
above is available.</p>
<p>For this study, we extended PWS to compute the mutual information
rate for systems with diffusive dynamics, described by Langevin
equations. For such systems, the aforementioned conditions are
inherently fulfilled and PWS can be applied. Specifically, in a Langevin
system, both the input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding="application/x-tex">S_t</annotation></semantics></math>
and the output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>t</mi></msub><annotation encoding="application/x-tex">X_t</annotation></semantics></math>
are stochastic processes given by the solution to a stochastic
differential equation (SDE). Using stochastic integration schemes like
the Euler-Mayurama method, we can straightforwardly generate
realizations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
from the corresponding stochastic process. These realizations are
naturally time-discretized with the integration time step
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Î”</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math>.
For a time-discretized trajectory
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ’™</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbfit{x}=(x_1,\ldots,x_n)</annotation></semantics></math>,
the path weight
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo>âˆ£</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\ln{\mathrm{P}}(\mathbfit{x} \mid \mathbfit{s})</annotation></semantics></math>
isâ€”up to a Gaussian normalization constantâ€”given by the Onsager-Machlup
action <span class="citation" data-cites="1953.Onsager">Â [<a
href="#ref-1953.Onsager" role="doc-biblioref">27</a>]</span> </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ln</mi><mo>&#8289;</mo></mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’™</mi><mo>âˆ£</mo><mi>ğ’”</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>âˆ’</mi><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>âˆ’</mo><mn>1</mn></mrow></munderover><mfrac><mn>1</mn><mrow><mn>2</mn><mi mathvariant="normal">Î”</mi><mi>t</mi></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi mathvariant="normal">Î”</mi><msub><mi>x</mi><mi>i</mi></msub><mo>âˆ’</mo><msub><mi>v</mi><mi>i</mi></msub><mi mathvariant="normal">Î”</mi><mi>t</mi></mrow><mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mtext mathvariant="normal">const</mtext></mrow><annotation encoding="application/x-tex">\ln{\mathrm{P}}(\mathbfit{x} \mid \mathbfit{s}) =
    -\sum^{n-1}_{i=1} 
    \frac{1}{2\Delta t} 
    \left( \frac{\Delta x_i - v_i \Delta t}{\sigma(x_i)} \right)^2 + \text{const}</annotation></semantics></math>
<p> where we used
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Î”</mi><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ’</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta x_i = x_{i+1} - x_i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">v_i = f(x_i, s_i)</annotation></semantics></math>
is the deterministic drift, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïƒ</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\sigma(x_i)</annotation></semantics></math>
represents the white noise amplitude. This expression captures the
likelihood of a particular trajectory, given the stochastic dynamics of
the system, and serves as the path weight in the PWS computation.</p>
<h2 id="case-studies">Case Studies</h2>
<p>To investigate the conditions under which the Gaussian approximation
deviates from the exact mutual information rate, we conducted two case
studies. In both studies we compare the Gaussian approximation against
the exact mutual information rate, computed via PWS. In the first case
study we focus on a discrete linear system which is inspired by minimal
motifs of cellular signaling.</p>
<h3 id="sec:linsys">Discrete reaction system</h3>
<p>We consider a simple linear reaction system of two species,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
whose dynamics are governed by 4Â reactions </p>
<div id="eq:reac1">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">âˆ… &amp; â†’ S</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">S &amp; â†’ âˆ…</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">S &amp; â†’ S + X</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">X &amp; â†’ âˆ…</mtext><mspace width="0.167em"></mspace><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \label{eq:reac1}\text{âˆ… &amp; â†’  S} \\ 
    \label{eq:reac2}\text{S &amp; â†’  âˆ…} \\ 
    \label{eq:reac3}\text{S &amp; â†’  S + X} \\ 
    \label{eq:reac4}\text{X &amp; â†’  âˆ…}\,. 
\end{aligned}</annotation></semantics></math>
</div>
<p> The reaction system is linear because each reaction has at most one
reactant. The trajectories of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are correlated because the production rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
depends on the copy number of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>,
and therefore information is transferred from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.
This set of reactions can be interpreted as a simple motif for gene
expression where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is a transcription factor and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
represents the expressed protein. In steady state, the mean copy numbers
are given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>Îº</mi><msup><mi>Î»</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{s} = \kappa\lambda^{-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>Ï</mi><msup><mi>Î¼</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{x} = \bar{s}\rho\mu^{-1}</annotation></semantics></math>.</p>
<p>The exact stochastic dynamics of this reaction system can be
expressed by the chemical master equation <span class="citation"
data-cites="2007.vanKampen">Â [<a href="#ref-2007.vanKampen"
role="doc-biblioref">26</a>]</span>. This equation describes the
time-evolution of the discrete probability distribution over the
possible copy numbers of species
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
capturing the noise from the chemical reaction events. From this
description we can obtain the mutual information rate from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
without approximations using PWS <span class="citation"
data-cites="2023.Reinhardt">Â [<a href="#ref-2023.Reinhardt"
role="doc-biblioref">5</a>]</span>.</p>
<p>While the chemical master equation is an exact representation of the
reaction system, for large copy numbers the stochastic dynamics are
well-approximated by a linearized model around the steady state. The
resulting Langevin equations can be systematically derived from the
master equation using the LNA which yields </p>
<div id="eq:input-dynamics">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mover><mi>s</mi><mo accent="true">Ì‡</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>Îº</mi><mo>âˆ’</mo><mi>Î»</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>Ï</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î¼</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \label{eq:input-dynamics}\dot{s}(t) &amp;= \kappa - \lambda  s(t) + \eta_s(t) \\ 
    \dot {x}(t) &amp;= \rho s(t) - \mu x(t) + \eta_x(t)
    \label{eq:output-dynamics}
\end{aligned}</annotation></semantics></math>
</div>
<p> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
are continuous variables representing the copy numbers of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î·</mi><mi>s</mi></msub><mo>,</mo><msub><mi>Î·</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">\eta_s, \eta_x</annotation></semantics></math>
are independent delta-correlated white noise terms with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo><mo>=</mo><mn>2</mn><mi>Î»</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\langle \eta^2_s \rangle = 2\lambda\bar{s}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>x</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo><mo>=</mo><mn>2</mn><mi>Î¼</mi><mover><mi>x</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\langle \eta^2_x \rangle = 2\mu\bar{x}</annotation></semantics></math>,
see <a href="#app:gauss">app:gauss</a>.</p>
<p>The Gaussian approximation of the mutual information rate is derived
from the LNA description. Using this framework, Tostevin and Wolde <span
class="citation" data-cites="2009.Tostevin">Â [<a
href="#ref-2009.Tostevin" role="doc-biblioref">1</a>]</span> computed an
analytical expression for the mutual information rate of the motif in
units of
natsÂ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>s</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">s^{-1}</annotation></semantics></math>:
</p>
<div id="eq:tostevin">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mtext mathvariant="normal">Gaussian</mtext></msub><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mfrac><mi>Ï</mi><mi>Î»</mi></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">R_\text{Gaussian} = \frac{\lambda}{2} \left( \sqrt{1 + \frac{\rho}{\lambda}} - 1 \right) \,.
    \label{eq:tostevin}</annotation></semantics></math>
</div>
<p>More recently, Moor and Zechner <span class="citation"
data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span> have derived a different expression
for the mutual information rate of this reaction system by analytically
approximating the relevant filtering equation, which is derived from the
master equation, thus recognizing the discreteness of molecules. This
approach explicitly differentiates the contributions of individual
reactions to the noise amplitude of each component, while the LNA lumps
their contributions together. As we will discuss in more detail below,
separately accounting for the noise from each reaction separately better
captures the information transmitted via discrete systems, making this
â€œdiscrete approximationâ€ more accurate than the Gaussian approximation
for this case study. Nevertheless, the result is still based on an
approximation that is only accurate for large copy numbers. The
expression for the mutual information rate in the discrete approximation
appears remarkably similar to the expression obtained using the Gaussian
framework: </p>
<div id="eq:moor">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mtext mathvariant="normal">discrete</mtext></msub><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mn>2</mn><mspace width="0.222em"></mspace><mfrac><mi>Ï</mi><mi>Î»</mi></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">R_\text{discrete} = \frac{\lambda}{2} \left( \sqrt{1 + 2\ \frac{\rho}{\lambda}} - 1 \right)\,.
    \label{eq:moor}</annotation></semantics></math>
</div>
<p> Note that this equation only differs from <a
href="#eq:tostevin">eq:tostevin</a> by the additional factor 2 inside
the square root.</p>
<p>The naturalâ€”but incorrectâ€”expectation is that for large copy numbers
both approximations converge to the true mutual information rate.
However, the difference between <a href="#eq:tostevin">eq:tostevin</a>
and <a href="#eq:moor">eq:moor</a> already reveals that the two
approximations do not converge. Indeed, previous work shows that even in
the limit of infinite copy numbers, the Gaussian approximation only
yields a lower bound to the information rate, which is not tight <span
class="citation" data-cites="2023.Reinhardt 2023.Moor">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>,<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span>.</p>
<figure id="fig:linear">
<img src="../images/linear.svg"  />
<figcaption>The mutual information rate of a simple linear reaction
system defined by <a href="#eq:reac1">eq:reac1</a>, <a
href="#eq:reac2">eq:reac2</a>, <a href="#eq:reac3">eq:reac3</a> and <a
href="#eq:reac4">eq:reac4</a>. The black dots show the exact information
rate, computed with PWS. We compare both, the Gaussian approximation of
Tostevin and Wolde <span class="citation"
data-cites="2009.Tostevin">Â [<a href="#ref-2009.Tostevin"
role="doc-biblioref">1</a>]</span> and the discrete approximation of
Moor and Zechner <span class="citation" data-cites="2023.Moor">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>]</span> against the
exact result. In panel (a), we use parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Îº</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\kappa=100</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda = 1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¼</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mu = 1</annotation></semantics></math>
while varying
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ï</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>.
The mean output copy number is directly proportional to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ï</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>
with proportionality factor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Îº</mi><msup><mi>Î»</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\kappa\lambda^{-1}=100</annotation></semantics></math>.
In panel (b), we fix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ï</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\rho=10</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda = 1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¼</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mu = 1</annotation></semantics></math>,
and systematically vary
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>.
As a consequence, we vary the mean input copy number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>Îº</mi><msup><mi>Î»</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bar{s}=\kappa\lambda^{-1}</annotation></semantics></math>,
and simultaneously also the mean output copy number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>Ï</mi><msup><mi>Î¼</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup><mo>=</mo><mn>10</mn><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\bar{x}=\bar{s}\rho\mu^{-1}=10\bar{s}</annotation></semantics></math>.
</figcaption>
</figure>
<p>We compare both approximations against exact PWS simulations for
different parameters. In <a href="#fig:linear">Fig. linear</a>a, we vary
the mean copy number of the readout,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math>,
by varying its synthesis rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ï</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>
and compute the mutual information rate using both approximations as
well as PWS, while keeping the input copy number constant at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\bar s = 100</annotation></semantics></math>.
We observe that the Gaussian approximation via the LNA [<a
href="#eq:tostevin">eq:tostevin</a>] consistently underestimates the
mutual information rate. This confirms that even when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar{s}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math>
are large, the Gaussian approximation only yields a lower bound to the
information rate of the discrete linear system. In contrast, the
discrete approximation [<a href="#eq:moor">eq:moor</a>] coincides with
the true mutual information rate obtained from PWS simulations over all
output copy numbers
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar x</annotation></semantics></math>,
even for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>â‰ª</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\bar x \ll 1</annotation></semantics></math>.</p>
<p>In <a href="#fig:linear">Fig. linear</a>b, instead of varying the
copy number of the output, we vary the copy number of the input by
varying the production rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>.
Note that both the Gaussian approximation and the discrete approximation
are independent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>.
Yet, we observe that the true mutual information rate is not. For
sufficiently large input copy numbers the discrete approximation
coincides with the true information rate while the Gaussian information
rate remains only a lower bound. Thus, the discrete approximation is
highly accurate for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>â‰¥</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\bar s \geq 10</annotation></semantics></math>.
For small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>&lt;</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\bar s &lt; 10</annotation></semantics></math>,
we find that the mutual information rate deviates from both, the LNA as
well as the discrete approximation. Surprisingly, we find an optimal
value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>
for which the mutual information rate is maximized and exceeds both
approximations. This implies that at low input copy numbers, the system
is able to extract additional information from the discrete input
trajectories, which is not accounted for by either of the
approximations.</p>
<p>In all cases, we found that the Gaussian approximation deviates
significantly from the true information rate for this discrete system.
Seemingly paradoxically, the Gaussian approximation based on the LNA
does not converge to the true information rate at high copy numbers,
even though the LNA approximates the stochastic dynamics extremely well
in this regime. In contrast, the discrete approximation from Moor and
Zechner <span class="citation" data-cites="2023.Moor">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>]</span> does not suffer
from this issue. It has been shown that, generally, the Gaussian
approximation is a lower bound on the discrete approximation <span
class="citation" data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span>, prompting the question of which
features of the discrete trajectories are not captured by the Gaussian
approximation. <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<h3 id="sec:nonlinsys">Nonlinear continuous system</h3>
<p>Next, we study a nonlinear variant of the reaction system above. In
contrast to the previous case study, we deliberately avoid using
discrete dynamics, as we already observed that the Gaussian
approximation is generally inaccurate in such systems. Instead, we focus
solely on continuous Langevin dynamics to explore how an explicitly
nonlinear input-output mapping affects the accuracy of the inherently
linear Gaussian approximation. We hypothesize that the accuracy of the
Gaussian approximation will deteriorate as the degree of nonlinearity
increases. To test this hypothesis, we analyze a simple Langevin system
with adjustable nonlinearity.</p>
<p>The system is defined by two coupled Langevin equations, one that
describes the input, and one that describes the output. The stochastic
dynamics of the input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>
are given by <a href="#eq:input-dynamics">eq:input-dynamics</a>. The
output dynamics of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
are given by </p>
<div id="eq:nonlinear_x">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Ï</mi><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î¼</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\dot {x}(t) = \rho a(s) - \mu x(t) + \eta_x(t) \label{eq:nonlinear_x}</annotation></semantics></math>
</div>
<p> with the Hill function </p>
<div id="eq:hillfunc">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mfrac><msup><mi>s</mi><mi>n</mi></msup><mrow><msup><mi>K</mi><mi>n</mi></msup><mo>+</mo><msup><mi>s</mi><mi>n</mi></msup></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>s</mi><mo>â‰¥</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>0</mn></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if </mtext><mspace width="0.333em"></mspace></mrow><mi>s</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">a(s) =
    \begin{cases}
        \frac{s^n}{K^n + s^n} &amp; \text{if } s \geq 0 \\
        0 &amp; \text{if } s&lt;0
    \end{cases}
     \,.
     \label{eq:hillfunc}</annotation></semantics></math>
</div>
<p> This function serves as a tuneable non-linearity with the Hill
coefficient
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.
As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>â†’</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n\to0</annotation></semantics></math>,
the Hill function approaches a shallow linear mapping, while for large
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
it becomes sigmoidal and highly non-linear. As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>â†’</mo><mi>âˆ</mi></mrow><annotation encoding="application/x-tex">n\to\infty</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>
approaches the unit step function centered at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">s=K</annotation></semantics></math>.
The so-called static input-output relation specifies the mean output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\bar{x}(s)</annotation></semantics></math>
for a given input signal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
and is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Ï</mi><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mi>Î¼</mi></mrow><annotation encoding="application/x-tex">\bar{x}(s) = \rho a(s) / \mu</annotation></semantics></math>.
The gain of this system is then defined as the slope of this relation at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">s=\bar{s}</annotation></semantics></math>,
i.e., </p>
<div id="eq:definition-gain">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><mfrac><mrow><mi>âˆ‚</mi><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>âˆ‚</mi><mi>s</mi></mrow></mfrac><msub><mo minsize="240%" maxsize="240%" stretchy="true" form="prefix">|</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></msub><mo>=</mo><mfrac><mrow><mi>n</mi><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>âˆ’</mo><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">]</mo><mi>Ï</mi></mrow><mrow><mi>Î¼</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">g= \frac{\partial \bar{x}(s)}{\partial s} \bigg|_{\bar{s}} = 
\frac{n a(\bar{s}) [1 - a(\bar{s})] \rho}{\mu\bar{s}} \,,
\label{eq:definition-gain}</annotation></semantics></math>
</div>
<p> as derived in <a href="#sec:gaussian-info-rate">Sec.
gaussian-info-rate</a>. Importantly, for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">\bar{s} = K</annotation></semantics></math>,
the gain of the system is directly proportional to the Hill
coefficientÂ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
i.e., the gain is directly coupled to the degree of nonlinearity.</p>
<p><a href="#fig:dynamical_input_output">Figure
dynamical_input_output</a> shows how, on average, the output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
at a given time depends on the input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>
at that same time (solid colored curves). This is the so-called
dynamical input-output relation of a system <span class="citation"
data-cites="2021.Malaguti">Â [<a href="#ref-2021.Malaguti"
role="doc-biblioref">29</a>]</span>. The solid black curve represents
the static input-output relation. While the static input-output relation
is purely determined by the instantaneous function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>,
the dynamical input-output relation depends not only on this function,
but also on the timescale of the response
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ï„</mi><mi>x</mi></msub><mo>=</mo><msup><mi>Î¼</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\tau_x = \mu^{-1}</annotation></semantics></math>.
The reason is that as the output responds more slowly to the input, the
temporal input fluctuations are averaged out increasingly. Therefore,
the response of the output becomes shallower for increasing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ï„</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\tau_x</annotation></semantics></math>.
Moreover, slower systems with more shallow responses react approximately
linear to the input (<a href="#fig:dynamical_input_output">Fig.
dynamical_input_output</a>).</p>
<figure id="fig:dynamical_input_output">
<img src="../images/dynamical_input_output.svg"  />
<figcaption>The dynamical input output relationship of the non-linear
system with a fixed static gain of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">g=5</annotation></semantics></math>
[<a href="#eq:definition-gain">eq:definition-gain</a>]. The upper panel
shows the static input-output relationship (solid black line) as well as
the dynamical input output relationship, i.e., the effective mapping
from input to output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>â†¦</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">S(t)\mapsto X(t)</annotation></semantics></math>
(at the same time) for different output timescales
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ï„</mi><mi>x</mi></msub><mo>=</mo><msup><mi>Î¼</mi><mrow><mi>âˆ’</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\tau_x = \mu^{-1}</annotation></semantics></math>.
The dynamical input-output mapping is defined as the conditional
expectation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext mathvariant="normal">dyn</mtext></msub><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>ğ”¼</mi><mo stretchy="false" form="prefix">[</mo><mi>X</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ£</mo><mi>S</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>s</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">x_\text{dyn}(s)=\mathbb{E}[X(t) \mid S(t)=s]</annotation></semantics></math>
and was estimated non-parametrically from simulated trajectories of the
system via Nadarayaâ€“Watson kernel regression <span class="citation"
data-cites="1964.Nadaraya 1964.Watson">Â [<a href="#ref-1964.Nadaraya"
role="doc-biblioref">30</a>,<a href="#ref-1964.Watson"
role="doc-biblioref">31</a>]</span> with a Gaussian kernel
(bandwidthÂ <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">h=\num{0.5}</annotation></semantics></math>).
Additionally, using the linear noise approximation we obtain linear
input output mappings with a dynamical gain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>g</mi><mo accent="true">Ìƒ</mo></mover><mo>=</mo><mi>g</mi><mi>/</mi><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>+</mo><msub><mi>Ï„</mi><mi>x</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tilde{g}=g/(1+\tau_x)</annotation></semantics></math>
(see <a href="#sec:gaussian-info-rate">Sec. gaussian-info-rate</a>)
which are displayed as dashed lines. We observe that the linear mapping
approximates the dynamical input output relation well for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>â‰ˆ</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">s\approx\bar{s}=100</annotation></semantics></math>
but cannot capture the nonlinear saturation effect. The lower panel
shows the stationary distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>.</figcaption>
</figure>
<figure id="fig:nonlinear">
<img src="../images/nonlinear.svg"  />
<figcaption>The information rate of a non-linear system as a function of
its gain over a range of response timescales. We vary the
<strong>static</strong> gain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
by varying the Hill coefficient
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
see <a href="#eq:definition-gain">eq:definition-gain</a>. A short
response timescale corresponds to a fast system (purple) while a long
response timescale corresponds to a slow system (yellow). The
information rate was computed in three different ways:
(<strong>a</strong>) via the Gaussian approximation using the LNA to
estimate the required power spectra; (<strong>b</strong>) via the
Gaussian approximation using simulations to numerically estimate the
required power spectra; (<strong>c</strong>) exactly via
PWS.</figcaption>
</figure>
<p>While using the Langevin extension of PWS we can directly compute the
mutual information rate of this nonlinear model, the Gaussian
approximation can only be applied to linear systems. Therefore, to
obtain the mutual information rate in the Gaussian approximation, we
have to linearize the system. There are two approaches for linearizing
the stochastic dynamics of this nonlinear system which result in
different information estimates.</p>
<p>The first approach is to linearize <a
href="#eq:nonlinear_x">eq:nonlinear_x</a> analytically via the LNA as
shown in <a href="#app:gauss:lna">app:gauss:lna</a>. Within this
approach we can obtain an analytical expression for the information rate
(see <a href="#sec:gaussian-info-rate">Sec. gaussian-info-rate</a>),
</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mtext mathvariant="normal">LNA</mtext></msub><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>g</mi><mn>2</mn></msup><mfrac><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>Î¼</mi></mrow><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>Î»</mi></mrow></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">R_\text{LNA} = \frac{\lambda}{2}\left(\sqrt{1+g^2 \frac{\bar s \mu}{\bar x \lambda}}-1\right).</annotation></semantics></math>
<p> This LNA based approach also yields a linearized dynamic
input-output relation, shown as dashed lines in <a
href="#fig:dynamical_input_output">Fig. dynamical_input_output</a>.</p>
<p>We observe that the linearized input-output relation closely matches
the slope of the true nonlinear dynamical input-output relation at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">s=\bar s = 100</annotation></semantics></math>,
but overall it does not correspond to a (least-squares) linear fit of
the nonlinear dynamical input-output relation. For all values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>,
the linearized input-output relation has a slope greater than or equal
to the slope of the dynamical input-output relation. Empirically, the
LNA thus seems to over-estimate the dynamical gain of the system. The
reason may be that the LNA approximates the static input-relation (<a
href="#fig:dynamical_input_output">Fig. dynamical_input_output</a> black
curve), and estimates the linearized dynamical input-output relation
based on this static approximation only.</p>
<p>The second â€œempirical Gaussianâ€ approach to linearize the nonlinear
system potentially avoids these issues. In this approach, we first
numerically generate trajectories from the stochastic <a
href="#eq:input-dynamics">eq:input-dynamics</a> and <a
href="#eq:nonlinear_x">eq:nonlinear_x</a> and use digital signal
processing techniques to estimate the mutual information rate from the
trajectories. We numerically estimate the (cross) power spectra of input
and response using Welchâ€™s method <span class="citation"
data-cites="1975.Oppenheim">Â [<a href="#ref-1975.Oppenheim"
role="doc-biblioref">32</a>, see Ch.Â 11]</span>. From the estimated
spectral densities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>S</mi><mo accent="true">Ì‚</mo></mover><mrow><mi>Î±</mi><mi>Î²</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\hat{S}_{\alpha\beta}(\omega)</annotation></semantics></math>
we compute the coherence </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>Ï•</mi><mo accent="true">Ì‚</mo></mover><mrow><mi>s</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mo stretchy="false" form="prefix">|</mo><msub><mover><mi>S</mi><mo accent="true">Ì‚</mo></mover><mrow><mi>s</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><mrow><msub><mover><mi>S</mi><mo accent="true">Ì‚</mo></mover><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msub><mover><mi>S</mi><mo accent="true">Ì‚</mo></mover><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat\phi_{sx}(\omega) = \frac{|\hat S_{sx}(\omega)|^2}{\hat S_{ss}(\omega) \hat S_{xx}(\omega)}</annotation></semantics></math>
<p> which we use to obtain the Gaussian approximation of the mutual
information rate directly using <a
href="#eq:gaussdef">eq:gaussdef</a>.</p>
<p>The empirical power spectra characterize the linear response of a
system, but not in the same way as the LNA. While for linear systems the
power spectra obtained via the LNA match the empirical power spectra
<span class="citation" data-cites="2006.Warren">Â [<a
href="#ref-2006.Warren" role="doc-biblioref">33</a>]</span>, for a
nonlinear system, the empirical power spectra and the coherence can
differ from the corresponding LNA calculations. The two linearization
approaches are thus not equivalent. We tested the accuracy of the
Gaussian mutual information rate estimates using both linearization
approaches to elucidate the differences in these approaches.</p>
<p><a href="#fig:nonlinear">Figure nonlinear</a> displays the mutual
information rate obtained via two linearized approximations as well as
the exact PWS result. We vary the gain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
and the response time-scale
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Ï„</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\tau_x</annotation></semantics></math>,
both of which significantly affect the shape of the dynamical
input-output relationship. As expected, a larger gain or a faster
response time lead to an increase in the mutual information rate. At
large gain, the information rate naturally saturates as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>
approaches a step function. The saturation effect is clearly seen in the
PWS results, and is found to be even more pronounced in the empirical
Gaussian approximation. The LNA-based Gaussian approximation, however,
shows no saturation. This highlights that the LNA linearizes the system
at the level of the input-output mapping
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>
which results in an approximation that is unaffected by the sigmoidal
shape of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>.
In contrast, the empirical approximation is affected by nonlinear
saturation effects because it is computed directly from simulated
trajectories. We thus see that both approximations yield substantially
different results at large gain.</p>
<figure id="fig:lna_pws_absolute">
<img src="../images/lna_pws_absolute.svg"  />
<figcaption>Deviation from the exact information rate for the
approximate Gaussian information rate computed via LNA (top), and the
approximate Gaussian information rate computed via empirical estimates
of the power spectra (bottom). A deviation of 0 implies perfect
accuracy. While the absolute deviation of both approximations increases
with increasing gain and decreasing response timescale, the LNA based
approach consistently overestimates the information rate whereas the
empirical approach constitutes a lower bound. Moreover, in terms of
absolute deviation, the empirical approach is more accurate across all
parameter values.</figcaption>
</figure>
<p>In <a href="#fig:lna_pws_absolute">Fig. lna_pws_absolute</a> we
compare the absolute deviation between the approximations and the PWS
result. For small gain we see that both approximations are accurate
which is not surprising since the nonlinearity is very weak in this
regime. Strikingly, for large gain, the LNA-based approximation always
overestimates the mutual information rate while the empirical Gaussian
approximation always underestimates the rate. In both cases the
systematic error decreases as the response timescale becomes slower.
This reflects the fact that for slow responders, the dynamic
input-output relation is more linear (<a
href="#fig:dynamical_input_output">Fig. dynamical_input_output</a>) than
for fast responders.</p>
<p>Additionally, we computed the relative deviation, see <a
href="#fig:lna_pws_ratio">Fig. lna_pws_ratio</a> in <a
href="#sec:relative-deviation">Sec. relative-deviation</a>. We find that
in terms of relative error the curves for different response timescales
largely overlap. In terms of relative approximation error, the gain,
rather than response timescale, is the primary factor affecting the
accuracy of the Gaussian approximation.</p>
<h2 id="discussion">Discussion</h2>
<p>We investigated the accuracy of the Gaussian approximation for the
mutual information rate in two case studies, each highlighting a
scenario where the approximation may be inaccurate. We were able to
reliably quantify the inaccuracy in each case by computing the â€œground
truthâ€ mutual information rate for these scenarios using a recently
developed exact Monte Carlo technique called PWS <span class="citation"
data-cites="2023.Reinhardt">Â [<a href="#ref-2023.Reinhardt"
role="doc-biblioref">5</a>]</span>.</p>
<p>We first considered linear discrete systems, which are relevant in
biology due to the discrete nature of biochemical signaling networks. In
our example, the Gaussian approximation cannot capture the full
information rate, but only yields a lower bound. We show that a discrete
approximation, developed by Moor and Zechner <span class="citation"
data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span>, is able to correctly estimate the
mutual information rate of the network over a wide range of parameters.
Since the Gaussian approximation captures the second moments of the
discrete system, this finding demonstrates that a discrete system can
transmit significantly more information than what would be inferred from
its second moments alone. This perhaps surprising fact has been observed
before <span class="citation"
data-cites="2019.Cepeda-Humerez 2023.Moor 2023.Reinhardt">Â [<a
href="#ref-2023.Moor" role="doc-biblioref">4</a>,<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>,<a
href="#ref-2019.Cepeda-Humerez" role="doc-biblioref">34</a>]</span> and
it hinges on the use of a discrete reaction-based readout. As
demonstrated in unpublished work <span class="citation"
data-cites="2024.Moor">Â [<a href="#ref-2024.Moor"
role="doc-biblioref">28</a>]</span>, the increased mutual information
rate found for a discrete readout stems from the ability of
unambiguously distinguishing between individual reaction events in the
readoutâ€™s trajectory. However, it remains an open question whether
biological (or other) signaling systems can effectively harness this
additional information encoded in the discrete trajectories. For systems
that cannot distinguish individual reaction events in downstream
processing, the Gaussian framework might still accurately quantify the
â€œaccessible informationâ€.</p>
<p>A notable new observation in our first case study is the deviation
between the discrete approximation of the mutual information rate
derived by Moor and Zechner <span class="citation"
data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span> and the exact result obtained using
PWS <span class="citation" data-cites="2023.Reinhardt">Â [<a
href="#ref-2023.Reinhardt" role="doc-biblioref">5</a>]</span> for inputs
with low copy number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar s</annotation></semantics></math>.
In the discrete approximation, the mutual information rate is
independent of the input copy number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar s</annotation></semantics></math>,
but the PWS simulations show that at low copy numbers there is an
optimal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mo>â‹†</mo></msup><mo>â‰ˆ</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">s^\star \approx 1</annotation></semantics></math>
which maximizes the mutual information rate. This surprising finding
suggests that the information rate in discrete systems can be increased
by reducing the copy number of the input sufficiently, such that it only
switches between a few discrete input levels. Notably, in the reverse
caseâ€”low output copy number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar x</annotation></semantics></math>
but large
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar s</annotation></semantics></math>â€”the
discrete approximation always remains accurate. We leave a precise
characterization of this finding for future work.</p>
<p>The second example focused on a continuous but nonlinear system,
where we demonstrated that the accuracy of the Gaussian approximation
depends on the linearization method. Linearizing the underlying system
dynamics directly via the LNA leads to an overestimation of the
information rate, while estimating the systemâ€™s correlation functions
empirically from data underestimates it. Regardless of the method, the
Gaussian approximation is more accurate in terms of absolute deviation
of the true information rate when the gain of the system is small and
its response is slow compared to the timescale of the input
fluctuations.</p>
<p>The result of our second case studyâ€”that the empirical Gaussian
mutual information rate underestimates the true rateâ€”is consistent with
theoretical expectations. As shown by Mitra and Stark <span
class="citation" data-cites="mitra2001nonlinear">Â [<a
href="#ref-mitra2001nonlinear" role="doc-biblioref">23</a>]</span>, and
highlighted in Ref.Â <span class="citation"
data-cites="2010.Tostevin">Â [<a href="#ref-2010.Tostevin"
role="doc-biblioref">2</a>]</span>, an empirical Gaussian estimate of
the mutual information between a Gaussian input signal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mi>G</mi></msub><annotation encoding="application/x-tex">S_G</annotation></semantics></math>
and a non-Gaussian output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
provides a lower bound on the channel capacity
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mrow><mi>max</mi><mo>&#8289;</mo></mrow><mrow><mi mathvariant="normal">P</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><mi>I</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">C(S,X)=\max_{\mathrm{P}(s)} I(S, X)</annotation></semantics></math>
(subject to a power constraint on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>).
Specifically, they show that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo stretchy="false" form="prefix">(</mo><mi>S</mi><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¥</mo><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>;</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mo>â‰¥</mo><mi>I</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>;</mo><msub><mi>X</mi><mi>G</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">C(S, X) \geq I(S_G; X) \geq I(S_G; X_G)</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>,</mo><msub><mi>X</mi><mi>G</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(S_G, X_G)</annotation></semantics></math>
is a jointly Gaussian pair with the same covariance matrix as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>,</mo><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(S_G, X)</annotation></semantics></math>.
For purely Gaussian systems like
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>S</mi><mi>G</mi></msub><mo>,</mo><msub><mi>X</mi><mi>G</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(S_G, X_G)</annotation></semantics></math>,
the mutual information calculated using <a
href="#eq:gaussdef">eq:gaussdef</a> is exact and equal to the channel
capacity. However, for systems that have a Gaussian input but are
otherwise non-Gaussian, the mutual information is larger or equal than
the corresponding Gaussian model with matching second moments, as
evidenced in <a href="#fig:lna_pws_absolute">Fig. lna_pws_absolute</a>.
In general, the empirical Gaussian approximation yields a lower bound on
the mutual information of the nonlinear system with a Gaussian input
signal, as well as a lower bound on the channel capacity of the
nonlinear system.<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a></p>
<p>We can distill several concrete recommendations for the computation
of the information rate from our analysis. For linear discrete systems,
the Gaussian approximation yields a lower bound on the true information
rate which may accurately quantify the information available to systems
that cannot distinguish individual discrete events. Alternatively, the
reaction-based discrete approximation by Moor and Zechner <span
class="citation" data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span> is highly accurate, even when the
copy number of the output is extremely small. However, when the copy
number of the input becomes small
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â‰²</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\lesssim 10</annotation></semantics></math>),
both approximations break down and one must use an exact method. Exact
methods for obtaining the information rate of any stochastic
reaction-based systems are PWS <span class="citation"
data-cites="2023.Reinhardt">Â [<a href="#ref-2023.Reinhardt"
role="doc-biblioref">5</a>]</span> or brute-force numerical integration
of the stochastic filtering equation, as shown in <span class="citation"
data-cites="2023.Moor">Â [<a href="#ref-2023.Moor"
role="doc-biblioref">4</a>]</span>. For nonlinear continuous systems
with small gain one can safely use the Gaussian approximation, either
based on a linearization of the underlying dynamics or on empirically
estimated correlation functions. Moreover, when the slowest input
time-scale is more than a magnitude faster than the response time-scale,
the non-linear response of the system is â€œaveraged outâ€ by the quick
input fluctuations, and the Gaussian approximation yields accurate
results. In this case, using a Gaussian approximation based on empirical
correlation functions yields the most accurate result, and provides a
rigorous lower bound for the mutual information. Finally, if the system
is both highly nonlinear and has a fast response with respect to the
input, one must resort to an exact method like PWS. We hope that our
results will guide future research in determining the appropriate method
for computing the mutual information rate.</p>
<p>Overall, our results greatly increase the usefulness of the Gaussian
approximation for the information rate of non-Gaussian systems. The
Gaussian approximation remains a useful method that can be applied
directly and straightforwardly to experimental data. Here, we have
quantified the prerequisites to safely use this approach. Moreover, we
elucidate how an empirical Gaussian approximation constitutes a lower
bound on the true information rate for systems with a sufficiently large
input copy number.</p>
<h2 id="supplementary-information">Supplementary Information</h2>
<h3 id="app:gauss">Gaussian approximation</h3>
<p>Here we derive the analytical expressions for the Gaussian
information rate of the networks considered in the main text. To this
end we first discuss the dynamics of the input signal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and its power spectrum. Then, we perform a linear approximation of the
dynamics of the readout species
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and derive the approximate Gaussian information rate between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
for the nonlinear network. Finally, we derive the Gaussian information
rate of the linear network from our expression of the Gaussian
information rate of the nonlinear network.</p>
<h4 id="signal">Signal</h4>
<p>The input signal is generated by a birth-death process, </p>
<div id="eq:SBD">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">âˆ… &amp; â‡Œ S</mtext><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:SBD}
\begin{aligned}
    \text{âˆ… &amp; â‡Œ  S},\\
\end{aligned}</annotation></semantics></math>
</div>
<p> Its dynamics in Langevin form are, </p>
<div id="eq:sigdyn">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">Ì‡</mo></mover><mo>=</mo><mi>Îº</mi><mo>âˆ’</mo><mi>Î»</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\dot{s} = \kappa -  \lambda s(t)+ \eta_s(t), \label{eq:sigdyn}</annotation></semantics></math>
</div>
<p> yielding the steady state signal concentration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>Îº</mi><mi>/</mi><mi>Î»</mi></mrow><annotation encoding="application/x-tex">\bar s=\kappa/\lambda</annotation></semantics></math>.
The independent Gaussian white noise process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î·</mi><mi>s</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\eta_s(t)</annotation></semantics></math>
summarizes all reactions that contribute to fluctuations in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>.
The strength of the noise term in steady state is </p>
<div id="eq:etas">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo><mo>=</mo><mi>Îº</mi><mo>+</mo><mi>Î»</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>2</mn><mi>Î»</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>.</mi></mrow><annotation encoding="application/x-tex">\langle \eta_s^2 \rangle = \kappa+ \lambda\bar s=2\lambda \bar s. \label{eq:etas}</annotation></semantics></math>
</div>
<p>The power spectral density, or power spectrum, of a stationary
process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math>
is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>=</mo><msub><mrow><mi>lim</mi><mo>&#8289;</mo></mrow><mrow><mi>T</mi><mo>â†’</mo><mi>âˆ</mi></mrow></msub><mfrac><mn>1</mn><mi>T</mi></mfrac><mo stretchy="false" form="prefix">|</mo><mover><msub><mi>x</mi><mi>T</mi></msub><mo accent="true">Ìƒ</mo></mover><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S_{xx} = \lim_{T \to \infty}\frac{1}{T} |\tilde{x_T}|^2</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">Ìƒ</mo></mover><annotation encoding="application/x-tex">\tilde{x}</annotation></semantics></math>
denotes the Fourier transform of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>.
The power spectrum of a signal obeying <a
href="#eq:sigdyn">eq:sigdyn</a> is thus given by </p>
<div id="eq:specss">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo></mrow><mrow><msup><mi>Ï‰</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Î»</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><mrow><msup><mi>Ï‰</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Î»</mi><mn>2</mn></msup></mrow></mfrac><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:specss}
\begin{aligned}
    S_{ss} 
    %&amp;= \lim_{T\to \infty} \frac{\kappa^2\delta(-\omega) \delta(\omega)/T + \f[-]{\eta_s}\tilde{\eta_s}/T}{\omega^2+\lambda^2}, \\
    &amp;= \frac{\langle \eta_s^2 \rangle}{\omega^2+\lambda^2}=\frac{2 \lambda \bar s}{\omega^2+\lambda^2}. 
\end{aligned}</annotation></semantics></math>
</div>
<h4 id="app:gauss:lna">Linear approximation</h4>
<p>We now consider the readout
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
which is produced via a nonlinear activation function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>:
</p>
<div id="eq:StoV">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">S &amp; â†’ S + X</mtext><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">X &amp; â†’ âˆ…</mtext><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:StoV}
\begin{aligned}
    \text{S &amp; â†’  S + X},\\
    \text{X &amp; â†’  âˆ…}.
\end{aligned}</annotation></semantics></math>
</div>
<p> We define the activation level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>
to be a Hill function, </p>
<div id="eq:as">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>n</mi></msup></mrow><mrow><msup><mi>K</mi><mi>n</mi></msup><mo>+</mo><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>n</mi></msup></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\label{eq:as}
    a(s) = \frac{s(t)^n}{K^n + s(t)^n}.</annotation></semantics></math>
</div>
<p> Such a dependency, in which
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
sets the concentration of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
at which the activation is half-maximal and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
sets the steepness, can for example arise from cooperativity between the
signal molecules in activating the synthesis of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.</p>
<p>We have for the dynamics of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
in Langevin form </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mo>=</mo><mi>Ï</mi><mspace width="0.167em"></mspace><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î¼</mi><mspace width="0.167em"></mspace><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\dot{x} =  \rho \, a(s) - \mu \, x(t) + \eta_x(t),</annotation></semantics></math>
<p> with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">a(s)</annotation></semantics></math>
given by <a href="#eq:as">eq:as</a>. The steady state concentration of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mi>Ï</mi><mi>/</mi><mi>Î¼</mi></mrow><annotation encoding="application/x-tex">\bar x = \bar a \rho/\mu</annotation></semantics></math>,
where we have defined the steady state activation level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>a</mi><mo stretchy="false" form="prefix">(</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\bar a = a(\bar s)</annotation></semantics></math>.
It is useful to determine the static gain of the network, which is
defined as the change in the steady state of the output upon a change in
the steady state of the signal: </p>
<div id="eq:g">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>g</mi></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msub><mi>âˆ‚</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mi>r</mi><mi>/</mi><mi>Î¼</mi><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>n</mi><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="postfix">)</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>/</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:g}
\begin{aligned}
    g &amp;= \partial_{\bar s} \bar x = r/\mu, \\ 
    &amp; = n(1-\bar a) \bar x/\bar s,
\end{aligned}</annotation></semantics></math>
</div>
<p> where we have defined the approximate linear activation rate </p>
<div id="eq:r">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mi>n</mi><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo stretchy="false" form="postfix">)</mo><mi>Ï</mi><mi>/</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:r}
    r =  n \bar a(1-\bar a)\rho/\bar s,</annotation></semantics></math>
</div>
<p> and the steady state of the activation level is given by </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mfrac><msup><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>n</mi></msup><mrow><msup><mi>K</mi><mi>n</mi></msup><mo>+</mo><msup><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>n</mi></msup></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\bar a =\frac{\bar s^n}{K^n + \bar s^n}.</annotation></semantics></math>
<p> Generally, we assume that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">K=\bar s</annotation></semantics></math>,
which entails that in steady state the network is tuned to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\bar a = 1/2</annotation></semantics></math>.</p>
<p>To compute the Gaussian information rate we approximate the dynamics
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
to first order around
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar x</annotation></semantics></math>
via the classical linear noise approximation <span class="citation"
data-cites="2007.vanKampen">Â [<a href="#ref-2007.vanKampen"
role="doc-biblioref">26</a>]</span>. Within this approximation the
dynamics of the deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î´</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\delta x(t) = x(t) -\bar x</annotation></semantics></math>
are, </p>
<div id="eq:xlindyn">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î´</mi><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mo>=</mo><mi>r</mi><mspace width="0.167em"></mspace><mi>Î´</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î¼</mi><mspace width="0.167em"></mspace><mi>Î´</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:xlindyn}
    \delta\dot{ x} = r\,  \delta s(t)- \mu \, \delta x(t) + \eta_x(t),</annotation></semantics></math>
</div>
<p> with the synthesis rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>
given by <a href="#eq:r">eq:r</a>.</p>
<p>In the linear noise approximation the noise strength is a constant
given by the noise strength at steady state, </p>
<div id="eq:etax">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>x</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo><mo>=</mo><mi>Ï</mi><mover><mi>a</mi><mo accent="true">â€¾</mo></mover><mo>+</mo><mi>Î¼</mi><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mn>2</mn><mi>Î¼</mi><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>.</mi></mrow><annotation encoding="application/x-tex">\label{eq:etax}
    \langle \eta_x^2 \rangle = \rho \bar a + \mu \bar x = 2 \mu \bar x.</annotation></semantics></math>
</div>
<h4 id="sec:gaussian-info-rate" id="information-rate">Information
rate</h4>
<p>Following Tostevin &amp; Ten Wolde <span class="citation"
data-cites="2009.Tostevin 2010.Tostevin">Â [<a href="#ref-2009.Tostevin"
role="doc-biblioref">1</a>,<a href="#ref-2010.Tostevin"
role="doc-biblioref">2</a>]</span>, we can express the Gaussian
information rate as follows, </p>
<div id="eq:infratedef">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’®</mi><mo>;</mo><mi>ğ’³</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>4</mn><mi>Ï€</mi></mrow></mfrac><msubsup><mo>âˆ«</mo><mrow><mi>âˆ’</mi><mi>âˆ</mi></mrow><mi>âˆ</mi></msubsup><mi>d</mi><mi>Ï‰</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mfrac><mrow><mo stretchy="false" form="prefix">|</mo><mi>K</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><mrow><mo stretchy="false" form="prefix">|</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow></mfrac><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:infratedef}
    R(\mathcal S; \mathcal X) =  \frac{1}{4\pi}\int_{-\infty}^{\infty} d\omega \log \left(1+ \frac{|K(\omega)|^2}{|N(\omega)|^2}S_{ss}\right),</annotation></semantics></math>
</div>
<p> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mi>K</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">|K(\omega)|^2</annotation></semantics></math>
is the frequency dependent gain and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">|N(\omega)|^2</annotation></semantics></math>
is the frequency dependent noise of the output process
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ’³</mi><annotation encoding="application/x-tex">\mathcal X</annotation></semantics></math>.
If the intrinsic noise of the network is not correlated to the process
that drives the signal, the power spectrum of the network output obeys
the spectral addition rule <span class="citation"
data-cites="tanase2006signal">Â [<a href="#ref-tanase2006signal"
role="doc-biblioref">35</a>]</span>. In this case the frequency
dependent gain and noise can be identified directly from the power
spectrum of the output, because it takes the following form: </p>
<div id="eq:specxx">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>=</mo><mo stretchy="false" form="prefix">|</mo><mi>K</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup><msub><mi>S</mi><mrow><mi>s</mi><mi>s</mi></mrow></msub><mo>+</mo><mo stretchy="false" form="prefix">|</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\label{eq:specxx}
    S_{xx} = |K(\omega)|^2 S_{ss} + |N(\omega)|^2.</annotation></semantics></math>
</div>
<p> For a species
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
obeying <a href="#eq:xlindyn">eq:xlindyn</a>, we have </p>
<div id="eq:fgfn">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mo stretchy="false" form="prefix">|</mo><mi>K</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><msup><mi>r</mi><mn>2</mn></msup><mrow><msup><mi>Î¼</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Ï‰</mi><mn>2</mn></msup></mrow></mfrac><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mo stretchy="false" form="prefix">|</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mi>Ï‰</mi><mo stretchy="false" form="postfix">)</mo><msup><mo stretchy="false" form="prefix">|</mo><mn>2</mn></msup></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>x</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo></mrow><mrow><msup><mi>Î¼</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Ï‰</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>2</mn><mi>Î¼</mi><mover><mi>x</mi><mo accent="true">â€¾</mo></mover></mrow><mrow><msup><mi>Î¼</mi><mn>2</mn></msup><mo>+</mo><msup><mi>Ï‰</mi><mn>2</mn></msup></mrow></mfrac><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:fgfn}
    \begin{aligned}
    |K(\omega)|^2 &amp;= \frac{r^2}{\mu^2 + \omega^2}, \\
    |N(\omega)|^2 &amp;= \frac{\langle \eta_x^2 \rangle}{\mu^2 + \omega^2}= \frac{2 \mu \bar x}{\mu^2 + \omega^2}.
    \end{aligned}</annotation></semantics></math>
</div>
<p> The Wiener Khinchin theorem states that the power spectrum of a
stochastic process and its auto-correlation function are a Fourier
transform pair. We thus obtain for the variance in the readout,
substituting the frequency dependent gain and noise [<a
href="#eq:fgfn">eq:fgfn</a>] and the power spectrum of the signal [<a
href="#eq:specss">eq:specss</a>] in <a href="#eq:specxx">eq:specxx</a>
and taking the inverse Fourier transform at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>,
</p>
<div id="eq:varx">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Ïƒ</mi><mi>x</mi><mn>2</mn></msubsup><mo>=</mo><mi>g</mi><mover><mi>g</mi><mo accent="true">Ìƒ</mo></mover><msubsup><mi>Ïƒ</mi><mi>s</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>Ïƒ</mi><mrow><mi>x</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mrow><msup><mi>g</mi><mn>2</mn></msup><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><mrow><mn>1</mn><mo>+</mo><mi>Î»</mi><mi>/</mi><mi>Î¼</mi></mrow></mfrac><mo>+</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:varx}
    \sigma_x^2 = g\tilde{g}\sigma_s^2 + \sigma^2_{x|s}=\frac{g^2 \bar s}{1+\lambda/\mu} + \bar x,</annotation></semantics></math>
</div>
<p> where the signal variance equals its mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Ïƒ</mi><mi>s</mi><mn>2</mn></msubsup><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\sigma_s^2=\bar s</annotation></semantics></math>,
and the mean readout concentration sets the intrinsic noise
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Ïƒ</mi><mrow><mi>x</mi><mo stretchy="false" form="prefix">|</mo><mi>s</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">\sigma^2_{x|s}=\bar x</annotation></semantics></math>.
We further have the static gain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
given by <a href="#eq:g">eq:g</a>, and have defined the dynamical gain
</p>
<div id="eq:dg">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>g</mi><mo accent="true">Ìƒ</mo></mover><mo>â‰¡</mo><mfrac><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><mi>Î´</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mi>Î´</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">âŸ©</mo></mrow><msubsup><mi>Ïƒ</mi><mi>s</mi><mn>2</mn></msubsup></mfrac><mo>=</mo><mfrac><mi>g</mi><mrow><mn>1</mn><mo>+</mo><mi>Î»</mi><mi>/</mi><mi>Î¼</mi></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\label{eq:dg}
    \tilde{g} \equiv \frac{\langle \delta x(t) \delta s(t) \rangle}{\sigma_s^2}= \frac{g}{1+\lambda/\mu},</annotation></semantics></math>
</div>
<p> which is the slope of the mapping from the time-varying signal value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>
to the time-varying readout
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>;
for Gaussian systems
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">âŸ¨</mo><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">|</mo><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">âŸ©</mo><mo>=</mo><mover><mi>g</mi><mo accent="true">Ìƒ</mo></mover><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\langle x(t)|s(t) \rangle = \tilde{g} s(t)</annotation></semantics></math>
<span class="citation"
data-cites="2010.Tostevin 2021.Malaguti tjalma2024predicting">Â [<a
href="#ref-2010.Tostevin" role="doc-biblioref">2</a>,<a
href="#ref-tjalma2024predicting" role="doc-biblioref">19</a>,<a
href="#ref-2021.Malaguti" role="doc-biblioref">29</a>]</span>.</p>
<p>To solve the integral in <a href="#eq:infratedef">eq:infratedef</a>
we exploit that </p>
<div id="eq:infrate1root">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>âˆ«</mo><mrow><mi>âˆ’</mi><mi>âˆ</mi></mrow><mi>âˆ</mi></msubsup><mi>d</mi><mi>Ï‰</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msup><mi>Ï‰</mi><mn>2</mn></msup><mo>+</mo><msup><mi>a</mi><mn>2</mn></msup></mrow><mrow><msup><mi>Ï‰</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>Ï€</mi><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>âˆ’</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\label{eq:infrate1root}
    \int^\infty_{-\infty} d\omega \log\left( \frac{\omega^2 + a^2}{\omega^2 + b^2}\right) = 2\pi(a-b).</annotation></semantics></math>
</div>
<p> Substituting the frequency dependent gain and noise given in <a
href="#eq:fgfn">eq:fgfn</a> and the signal power spectrum of <a
href="#eq:specss">eq:specss</a> in <a
href="#eq:infratedef">eq:infratedef</a> and using <a
href="#eq:infrate1root">eq:infrate1root</a> we obtain the information
rate, </p>
<div id="eq:Rnl">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’®</mi><mo>;</mo><mi>ğ’³</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><msup><mi>r</mi><mn>2</mn></msup><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>s</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo></mrow><mrow><msup><mi>Î»</mi><mn>2</mn></msup><mo stretchy="false" form="prefix">âŸ¨</mo><msubsup><mi>Î·</mi><mi>x</mi><mn>2</mn></msubsup><mo stretchy="false" form="postfix">âŸ©</mo></mrow></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>g</mi><mn>2</mn></msup><mfrac><mrow><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>Î¼</mi></mrow><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>Î»</mi></mrow></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\label{eq:Rnl}
    \begin{aligned}
        R(\mathcal S; \mathcal X) &amp;= \frac{\lambda}{2}\left(\sqrt{1+\frac{r^2\langle \eta_s^2 \rangle}{\lambda^2\langle \eta_x^2 \rangle}}-1\right), \\
        &amp;= \frac{\lambda}{2}\left(\sqrt{1+g^2\frac{\bar s \mu}{\bar x \lambda}}-1\right),
    \end{aligned}</annotation></semantics></math>
</div>
<p> where we used the noise strengths given in <a
href="#eq:etas">eq:etas</a> and <a href="#eq:etax">eq:etax</a>, we have
the static gain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
of <a href="#eq:g">eq:g</a>, and the synthesis rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>
of <a href="#eq:r">eq:r</a>.</p>
<h4 id="linear-network">Linear network</h4>
<p>To disambiguate differences in the information rate caused by the
linear approximation of our nonlinear reaction network on one hand and
the Gaussian approximation of the underlying jump process on the other,
we consider the information rate of a linear network. Any difference
between the exact information rate and the Gaussian information rate
must then be a result of the Gaussian approximation. To this end we use
the same input signal [<a href="#eq:sigdyn">eq:sigdyn</a>], but we
consider a linear activation of the readout, i.e. </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">S &amp; â†’ S + X</mtext><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">X &amp; â†’ âˆ…</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{S &amp; â†’  S + X},\\
    \text{X &amp; â†’  âˆ…}
\end{aligned}</annotation></semantics></math>
<p> such that the Langevin dynamics of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are </p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">Ì‡</mo></mover><mo>=</mo><mi>Ï</mi><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>âˆ’</mo><mi>Î¼</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\dot x = \rho s(t) - \mu x(t) + \eta_x(t),</annotation></semantics></math>
<p> which yields the steady state concentration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mover><mi>s</mi><mo accent="true">â€¾</mo></mover><mi>Ï</mi><mi>/</mi><mi>Î¼</mi></mrow><annotation encoding="application/x-tex">\bar x= \bar s \rho/\mu</annotation></semantics></math>.
For this linear readout, the static gain is simply set by the ratio of
steady states of the input and the output,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><mi>Ï</mi><mi>/</mi><mi>Î¼</mi><mo>=</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>/</mi><mover><mi>s</mi><mo accent="true">â€¾</mo></mover></mrow><annotation encoding="application/x-tex">g=\rho/\mu=\bar x/\bar s</annotation></semantics></math>.
We can then obtain the information rate of this linear system by
substitution of its static gain in <a href="#eq:Rnl">eq:Rnl</a>, which
yields </p>
<div id="eq:Rlin">
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>ğ’®</mi><mo>;</mo><mi>ğ’³</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mfrac><mi>Ï</mi><mi>Î»</mi></mfrac></mrow></msqrt><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\label{eq:Rlin}
        R(\mathcal S; \mathcal X) %&amp;= \frac{\lambda}{2}\left(\sqrt{1+\frac{\bar x \mu}{\bar s \lambda}}-1\right), \\
        &amp; = \frac{\lambda}{2}\left(\sqrt{1+\frac{\rho}{\lambda}}-1\right).
        \end{aligned}</annotation></semantics></math>
</div>
<p> This result is identical to that of <span class="citation"
data-cites="2009.Tostevin 2010.Tostevin">Â [<a href="#ref-2009.Tostevin"
role="doc-biblioref">1</a>,<a href="#ref-2010.Tostevin"
role="doc-biblioref">2</a>]</span> (motif III).</p>
<h3 id="sec:relative-deviation">Relative deviation of the Gaussian
approximation for a nonlinear system</h3>
<figure id="fig:lna_pws_ratio">
<img src="../images/lna_pws_ratio.svg"  />
<figcaption>Relative deviation from the exact information rate for the
approximate Gaussian information rate. The relative deviation was
computed for both the LNA-based approximation and the empirical Gaussian
approximation using numerically estimated power spectra, across varying
system gains and response timescales (see <a href="#sec:nonlinsys">Sec.
nonlinsys</a> for details). The curves for different response timescales
largely overlap, indicating that the relative approximation error is
primarily influenced by system gain rather than response timescale. This
highlights that system gain is the dominant factor in determining the
accuracy of the Gaussian approximation.</figcaption>
</figure>
<p>Due to the definition of the mutual information, an absolute
difference in information maps to a relative difference in the reduction
of uncertainty. For this reason, <a href="#fig:lna_pws_absolute">Fig.
lna_pws_absolute</a> in the main text (<a href="#sec:nonlinsys">Sec.
nonlinsys</a>) focuses on the absolute deviation between the Gaussian
approximation and the true mutual information. We compared two variants
of the Gaussian approximation, the LNA-based approximation and the
empirical Gaussian approximation. We found that in both cases the
absolute deviation decreases with slower response timescales, reflecting
the more linear input-output relationship in slow-responding
systems.</p>
<p>However, the relative deviation of the Gaussian information rate from
the true rate also offers valuable insights, which we explore here. In
<a href="#fig:lna_pws_ratio">Fig. lna_pws_ratio</a> we compare the
relative deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mtext mathvariant="normal">Gaussian</mtext></msub><mi>/</mi><msub><mi>R</mi><mtext mathvariant="normal">PWS</mtext></msub></mrow><annotation encoding="application/x-tex">R_\text{Gaussian} / R_\text{PWS}</annotation></semantics></math>
between the Gaussian approximation and the exact mutual information
computed using PWS. We find that the relative deviation increases as the
system gain increases, indicating that the Gaussian approximation also
becomes relatively less accurate for larger gains. As already discussed
above, the empirical Gaussian method consistently underestimates the
true information rate, while the LNA-based approximation overestimates
it.</p>
<p>Interestingly, we also observe that for the LNA approximation, at
fast timescales the result is slightly more accurate, whereas the
empirical Gaussian estimate is more accurate at slow timescales. We
initially expected that in both cases slow timescales would yield better
agreement with PWS, as the input-output dynamics are more linear for
slow timescales, and thus better approximated by the Gaussian model. The
fact that this is not the case for the LNA approximation is intriguing,
indicating the need for further investigation into the interplay between
timescales, system nonlinearity, and the LNA.</p>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-2009.Tostevin" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">F.
Tostevin and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physrevlett.102.218101"><span
class="nocase">Mutual Information between Input and Output Trajectories
of Biochemical Networks</span></a>, Physical Review Letters
<strong>102</strong>, 218101 (2009).</div>
</div>
<div id="ref-2010.Tostevin" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">F.
Tostevin and P. R. ten Wolde, <a
href="https://doi.org/10.1103/PhysRevE.81.061917">Mutual information in
time-varying biochemical systems</a>, Phys. Rev. E <strong>81</strong>,
061917 (2010).</div>
</div>
<div id="ref-2021.Mattingly" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">H.
H. Mattingly, K. Kamino, B. B. Machta, and T. Emonet, <a
href="https://doi.org/10.1038/s41567-021-01380-3"><span
class="nocase">Escherichia coli chemotaxis is information
limited</span></a>, Nature Physics <strong>17</strong>, 1426
(2021).</div>
</div>
<div id="ref-2023.Moor" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div
class="csl-right-inline">A.-L. Moor and C. Zechner, <a
href="https://doi.org/10.1103/physrevresearch.5.013032"><span
class="nocase">Dynamic information transfer in stochastic biochemical
networks</span></a>, Physical Review Research <strong>5</strong>, 013032
(2023).</div>
</div>
<div id="ref-2023.Reinhardt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">M.
Reinhardt, G. TkaÄik, and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physrevx.13.041017"><span
class="nocase">Path Weight Sampling: Exact Monte Carlo Computation of
the Mutual Information between Stochastic Trajectories</span></a>,
Physical Review X <strong>13</strong>, 041017 (2023).</div>
</div>
<div id="ref-2023.Hahn" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">L.
Hahn, A. M. Walczak, and T. Mora, <a
href="https://doi.org/10.1103/physrevlett.131.128401"><span
class="nocase">Dynamical Information Synergy in Biochemical Signaling
Networks</span></a>, Physical Review Letters <strong>131</strong>,
128401 (2023).</div>
</div>
<div id="ref-1986.Segall" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">J.
E. Segall, S. M. Block, and H. C. Berg, <a
href="https://doi.org/10.1073/pnas.83.23.8987"><span
class="nocase">Temporal comparisons in bacterial chemotaxis</span></a>,
Proceedings of the National Academy of Sciences <strong>83</strong>,
8987 (1986).</div>
</div>
<div id="ref-covert2005achieving" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">M.
W. Covert, T. H. Leung, J. E. Gaston, and D. Baltimore, Achieving
stability of lipopolysaccharide-induced
NF-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Îº</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math>b
activation, Science <strong>309</strong>, 1854 (2005).</div>
</div>
<div id="ref-1998.Strong" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">S.
P. Strong, R. Koberle, R. R. de Ruyter van Steveninck, and W. Bialek, <a
href="https://doi.org/10.1103/physrevlett.80.197"><span
class="nocase">Entropy and Information in Neural Spike
Trains</span></a>, Physical Review Letters <strong>80</strong>, 197
(1998).</div>
</div>
<div id="ref-1948.Shannon" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">C.
E. Shannon, <a
href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x"><span
class="nocase">A Mathematical Theory of Communication</span></a>, Bell
System Technical Journal <strong>27</strong>, 379 (1948).</div>
</div>
<div id="ref-2007.Ziv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">E.
Ziv, I. Nemenman, and C. H. Wiggins, <a
href="https://doi.org/10.1371/journal.pone.0001077"><span
class="nocase">Optimal Signal Processing in Small Stochastic Biochemical
Networks</span></a>, PLoS ONE <strong>2</strong>, e1077 (2007).</div>
</div>
<div id="ref-2011.Cheong" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">R.
Cheong, A. Rhee, C. J. Wang, I. Nemenman, and A. Levchenko, <a
href="https://doi.org/10.1126/science.1204553"><span
class="nocase">Information Transduction Capacity of Noisy Biochemical
Signaling Networks</span></a>, Science <strong>334</strong>, 354
(2011).</div>
</div>
<div id="ref-dubuis2013positional" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J.
O. Dubuis, G. TkaÄik, E. F. Wieschaus, T. Gregor, and W. Bialek,
Positional information, in bits, Proceedings of the National Academy of
Sciences <strong>110</strong>, 16301 (2013).</div>
</div>
<div id="ref-2015.Palmer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">S.
E. Palmer, O. Marre, M. J. Berry, and W. Bialek, <a
href="https://doi.org/10.1073/pnas.1506855112"><span
class="nocase">Predictive information in a sensory
population</span></a>, Proceedings of the National Academy of Sciences
<strong>112</strong>, 6908 (2015).</div>
</div>
<div id="ref-chalk2018toward" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">M.
Chalk, O. Marre, and G. TkaÄik, Toward a unified theory of efficient,
predictive, and sparse coding, Proceedings of the National Academy of
Sciences <strong>115</strong>, 186 (2018).</div>
</div>
<div id="ref-bauer2021trading" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">M.
Bauer, M. D. Petkova, T. Gregor, E. F. Wieschaus, and W. Bialek, Trading
bits in the readout from a genetic network, Proceedings of the National
Academy of Sciences <strong>118</strong>, e2109011118 (2021).</div>
</div>
<div id="ref-sachdeva2021optimal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">V.
Sachdeva, T. Mora, A. M. Walczak, and S. E. Palmer, Optimal prediction
with resource constraints using the information bottleneck, PLoS
Computational Biology <strong>17</strong>, e1008743 (2021).</div>
</div>
<div id="ref-tjalma2023trade" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">A.
J. Tjalma, V. Galstyan, J. Goedhart, L. Slim, N. B. Becker, and P. R.
ten Wolde, Trade-offs between cost and information in cellular
prediction, Proceedings of the National Academy of Sciences
<strong>120</strong>, e2303078120 (2023).</div>
</div>
<div id="ref-tjalma2024predicting" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">A.
J. Tjalma and P. R. ten Wolde, Predicting concentration changes via
discrete receptor sampling, Physical Review Research <strong>6</strong>,
033049 (2024).</div>
</div>
<div id="ref-2006.Cover" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">T.
M. Cover and J. A. Thomas, <em><span class="nocase">Elements of
Information Theory</span></em>, 2nd ed. (John Wiley &amp; Sons,
2006).</div>
</div>
<div id="ref-2008.Tkacik" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">G.
TkaÄik, C. G. Callan, and W. Bialek, <a
href="https://doi.org/10.1073/pnas.0806077105"><span
class="nocase">Information flow and optimization in transcriptional
regulation</span></a>, Proceedings of the National Academy of Sciences
<strong>105</strong>, 12265 (2008).</div>
</div>
<div id="ref-deronde2010effect" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">W.
H. de Ronde, F. Tostevin, and P. R. ten Wolde, Effect of feedback on the
fidelity of information transmission of time-varying signals, Physical
Review Eâ€”Statistical, Nonlinear, and Soft Matter Physics
<strong>82</strong>, 031914 (2010).</div>
</div>
<div id="ref-mitra2001nonlinear" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">P.
P. Mitra and J. B. Stark, Nonlinear limits to the information capacity
of optical fibre communications, Nature <strong>411</strong>, 1027
(2001).</div>
</div>
<div id="ref-2021.Meijers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">M.
Meijers, S. Ito, and P. R. ten Wolde, <a
href="https://doi.org/10.1103/physreve.103.l010102"><span
class="nocase">Behavior of information flow near criticality</span></a>,
Physical Review E <strong>103</strong>, L010102 (2021).</div>
</div>
<div id="ref-2024.Fan" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">R.
Fan and A. Hilfinger, <a
href="https://doi.org/10.1103/physreve.110.034309"><span
class="nocase">Characterizing the nonmonotonic behavior of mutual
information along biochemical reaction cascades</span></a>, Physical
Review E <strong>110</strong>, 034309 (2024).</div>
</div>
<div id="ref-2007.vanKampen" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">N.
G. van Kampen, <em><a
href="https://doi.org/10.1016/b978-0-444-52965-7.x5000-4"><span
class="nocase">Stochastic Processes in Physics and
Chemistry</span></a></em>, 3rd ed. (Elsevier, Amsterdam, 2007).</div>
</div>
<div id="ref-1953.Onsager" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">L.
Onsager and S. Machlup, <a
href="https://doi.org/10.1103/physrev.91.1505"><span
class="nocase">Fluctuations and Irreversible Processes</span></a>,
Physical Review <strong>91</strong>, 1505 (1953).</div>
</div>
<div id="ref-2024.Moor" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div
class="csl-right-inline">A.-L. Moor, A. Tjalma, M. Reinhardt, P. R. ten
Wolde, and C. Zechner, <em>Reaction-Based Information Processing in
Biochemical Networks</em>, (unpublished).</div>
</div>
<div id="ref-2021.Malaguti" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">G.
Malaguti and P. R. ten Wolde, <a
href="https://doi.org/10.7554/elife.62574"><span class="nocase">Theory
for the optimal detection of time-varying signals in cellular sensing
systems</span></a>, eLife <strong>10</strong>, e62574 (2021).</div>
</div>
<div id="ref-1964.Nadaraya" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">E.
A. Nadaraya, <a href="https://doi.org/10.1137/1109020"><span>On
Estimating Regression</span></a>, Theory of Probability &amp; Its
Applications <strong>9</strong>, 141 (1964).</div>
</div>
<div id="ref-1964.Watson" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">G.
S. Watson, <span>Smooth Regression Analysis</span>, SankhyÄ: The Indian
Journal of Statistics, Series A (1961-2002) <strong>26</strong>, 359
(1964).</div>
</div>
<div id="ref-1975.Oppenheim" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">A.
V. Oppenheim and R. W. Schafer, <em><span>Digital Signal
Processing</span></em> (Prentice-Hall, Englewood Cliffs, New Jersey,
1975).</div>
</div>
<div id="ref-2006.Warren" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">P.
B. Warren, S. TÄƒnase-Nicola, and P. R. ten Wolde, <a
href="https://doi.org/10.1063/1.2356472"><span class="nocase">Exact
results for noise power spectra in linear biochemical reaction
networks</span></a>, The Journal of Chemical Physics
<strong>125</strong>, 144904 (2006).</div>
</div>
<div id="ref-2019.Cepeda-Humerez" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">S.
A. Cepeda-Humerez, J. Ruess, and G. TkaÄik, <a
href="https://doi.org/10.1371/journal.pcbi.1007290"><span
class="nocase">Estimating information in time-varying
signals.</span></a>, PLoS Computational Biology <strong>15</strong>,
e1007290 (2019).</div>
</div>
<div id="ref-tanase2006signal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">S.
TÄƒnase-Nicola, P. B. Warren, and P. R. ten Wolde, Signal detection,
modularity, and the correlation between extrinsic and intrinsic noise in
biochemical networks, Physical Review Letters <strong>97</strong>,
068102 (2006).</div>
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>In still unpublished work together with Anne-Lena Moor
and Christoph Zechner <span class="citation" data-cites="2024.Moor">Â [<a
href="#ref-2024.Moor" role="doc-biblioref">28</a>]</span>, we found that
the root cause for the deviations of the Gaussian approximation lies in
how the LNA approximates the reaction noise in the chemical master
equation. While the dynamics of the chemical master equation give rise
to discrete sample paths, i.e., piece-wise constant trajectories
connected by instantaneous discontinuous jumps, the LNA approximation
yields continuous stochastic trajectories. Our results imply that a
discrete sample path of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
carries more information about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
than the corresponding continuous sample path
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>
would carry about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">s(t)</annotation></semantics></math>
in the LNA. In the collaborative effort <span class="citation"
data-cites="2024.Moor">Â [<a href="#ref-2024.Moor"
role="doc-biblioref">28</a>]</span> we found that this is ultimately due
to the fact that in the discrete system, each reaction event is
unambiguously recorded in the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
trajectories and thus different reactions modifying the same species can
be distinguished. In contrast, in the continuous LNA description, all
reactions that modify
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
contribute to the noise term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î·</mi><mi>x</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\eta_x(t)</annotation></semantics></math>
in <a href="#eq:output-dynamics">eq:output-dynamics</a> but their
contributions are lumped together and therefore cannot be distinguished
from an observed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mi>t</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">x(t)</annotation></semantics></math>-trajectory.
Specifically, note that for the motif studied here, only the production
reaction
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>â†’</mo><mi>S</mi><mo>+</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">S\to S+X</annotation></semantics></math>
conveys information. The decay reaction of the output
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>â†’</mo><mi>âˆ…</mi></mrow><annotation encoding="application/x-tex">X\to\emptyset</annotation></semantics></math>
does not carry information on the input fluctuations since its
propensity is independent of the input. Yet, it contributes to the
overall fluctuations in the output. The Gaussian approximation only
considers the total fluctuations in the output, while the discrete
approximation correctly distinguishes between the fluctuations induced
from production events and decay events. Therefore, the Gaussian
approximation consistently underestimates the true information
transmission, whereas the discrete approximation does not incur this
systematic error. This subtle point is reflected in the difference
between <a href="#eq:tostevin">eq:tostevin</a> and <a
href="#eq:moor">eq:moor</a>.<a href="#fnref1" class="footnote-back"
role="doc-backlink">â†©ï¸</a></p></li>
<li id="fn2"><p>Note that this argument does not apply to the Linear
Noise Approximation (LNA). The bound specifically requires the Gaussian
model to use the covariance of the full, original system. When the
system is first linearized using the LNA, the resulting linear model
does not retain the same covariance as the original nonlinear system. As
a result, the mutual information rate calculated with the LNA is
generally not a lower (nor an upper) bound on the true mutual
information rate.<a href="#fnref2" class="footnote-back"
role="doc-backlink">â†©ï¸</a></p></li>
</ol>
</section>
</body>
</html>
